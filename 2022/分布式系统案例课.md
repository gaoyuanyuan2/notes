# 分布式系统案例课

## 分布式系统案例课

### 需求澄清

* 场景用例
  * 谁用这个系统?
  * 用户如何用这个系统?

* 量级规模(读/写)
  * 每秒查询请求?
  * 每个请求查询多少数据?
  * 每秒处理多少个视频观看记，录?
  * 流量模式?是否有流量高峰?
  
* 性能
  * 预期从写人到读取数据的延迟?
  * 预期p99读请求延迟是多少?
  * 高可用性(一般隐含)

* 成本
  * 开发成本有限制?
  * 运维成本有限制?


* 非功能需求
  * 规模 每秒处理1W+视频点击观看记录
  * 性能1 写入/读取毫秒级延迟
  * 性能2 写入到读取更新分钟级延迟,近实时流处理,最终一致
  * 高可用 无单点失败
  * 水平按需扩展
  * 开源低成本

### 数据库选型

* 可扩展~根据读写规模按需扩展
* 高性能~快速读写
* 高可用~不丢数据，灾难恢复
* 一致性折衷
* 数据模型易于升级
* 成本
* 开发者学习门槛


#### SQL数据库+客户端嵌入代理

ShardingSphere

* 分区存储
* 主从复制
* 读写分离

#### NoSQL数据库(Cassandra)

* 比较适合时间序列存储的场景
* Cassandra是支持最终一致的数据库
* 支持按需动态添加节点
* 可以线性扩容
* 原生是支持跨数据中心的

运维管理复杂性也不低

### 计数服务如何实现

* 可扩展~可根据写入规模按需扩展
* 高性能~快速写入，高吞吐
* 高可用~不丢数据，灾难恢复，数据库慢或者不可用? 

* 可扩展 = Partitioning/Sharding
* 高性能吞吐 = 内存计算,Batch批处理
* 高可靠 = 持久化 & Replication & checkpointing

数据聚合(aggregation)基础

每个请求都会对数据库当中A的观看数加1：简单，实时性好，频繁写入DB会造成性能问题

批量聚合运算，定期写入DB：提升吞吐量（多线程计算ConcurrentHashmap存储）
每分钟将计算结果写入到内部的另外一个队列，后台的DB可能会慢


大规模数据处理的时候，通常采用预聚合加上批处理的方式

为了解决高可用的问题，业界实践的做法通常在用户请求和计数服务之间增加一个消息队列MQ
拉模式可以缓冲生产和消费速度不匹配的问题，起到一个流量削峰的作用


API Gateway可以做路由转发、监控日志、限流容错

网关如何实现容错限流：引入Hystrix 容错限流组件

如何通过网关实现实时的防爬虫

服务发现：Nginx + DNS、Eureka注册中心加上Ribbon客户端

负载均衡：可以采用软件Nginx或者硬件F5

老数据归档

### 技术栈选型

API Gateway可以选用Netflix Zuul网关，或者用Spring Cloud Gateway

网关也是需要做集群和负载均衡的，可以选择硬件的F5，如果采用阿里云的话可以选择SLB负载均衡器

Spring Boot开发

采用Netflix Eureka

Netflix Ribbon做客户端软负载

MQ选主流的Kafka

分布式缓存选用Redis Cluster

数据存储选这个Cassandra，比较适合大规模时间序列数据的存储，也可以考虑用类似的Apache HBase

线下计算选用主流的Hadoop

如果次用阿里云长期存储可以选用对象存储OSS

### 进一步考量和总结

#### 然后定位系统瓶颈

压力测试找出潜在的性能内存和多线程问题

通过对小规模集群进行性能测试可以获得基准性能数据从而估算出为了应对生产的流量，计算出需要申请多少的软硬件资源

#### 如何监控系统的健康状况

需要对系统进行细粒度的埋点和监控，对核心服务的调用量，调用延迟错误数都要监控

对硬件资源包括CPU内存磁盘的利用率也要监控起来

对JVM的垃圾回收这些活动也要监控起来

队列也要监控起来，尤其是对这个消息堆积的情况要监控和告警，消息堆积是系统需要扩容的一个重要的信号

* 监控手段
  * 日志监控
  * Metrics监控
  * 调用链监控
  * 健康检查

#### 如何确保线上系统运行结果正确

需要对系统进行全面的线下的功能测试

普通的现下测试常常不能完全覆盖线上得真实流量

有些场景对这个数据的准确性要求是非常高的
比方说如果视频观看量涉及到广告费和作者的分成，那么计数服务它的准确性就不能有误差

一种做法是开发一套线上的模拟程序（实时流计算）

业界还有一种更重量级的做法：Lambda Architecture
就是在实时流计算的基础上，再引入一套线下的批处理系统，比方说Hadoop Map/ Reduce，两套系统同时做相同计算，后台有一个校验系统，对两套系统产生的结果进行校验
这种做法可以进步确保系统的功能正确性

#### 如何解决热分区的问题

对这个视频ID key这个部分添加时间戳，可以把热点视频按时间，比方说每小时分摊到不同分区当中


#### 如何监控慢消费者


## 如何设计一个简化版Kafka消息队列


### Kafka的动态重平衡是如何工作的？

为了减少不必要的重平衡数量，减少STOP-THE-WORLD它的影响，最近版本的Kafka对重平衡协议进行了优化扩展。

### 消息队列设计和治理最佳实践

消费者端拉模式是大规模企业级消息系统设计的最佳实践

泳道隔或者说舱壁隔离是系统设计的一个重要思想

延迟VS吞吐

动态重平衡协议

#### MQ治理最佳实践

* 研发自助治理
  * 主题/分区申请,扩容,监控
* 堆积(lag)监控告警
* 动态偏移调整
* 失败消息处理
  * 死信(dead letter) 队列
* 线上测试+监控

## 如何解决微服务的数据一致性分发问题？

* 拉模式有两个大问题
  * 拉数据通常会有延迟
  * 影响服务性能

* 数据分发场景
  * 数据复制(replication)
  * 支持数据库拆分迁移
  * 实现CQRS/去数据库Join
  * 实现分布式事务
  * 流计算
  * 大数据BI/AI
  * 审计日志/历史归档

发消息异常不一定失败了，可能网络抖动导致的异常

模式一:事务性发件箱(Transactional Outbox)
模式二:变更数据捕获(Change Data Capture, CDC)

## 如何解决微服务的数据聚合Join问题？

为了解决分布式聚合Jon慢的问题

通常采用另外一种数据分发加上预聚合的这种方式

通过K-V键查询的话，可以采用Redis

通过关键字查询的话，可以采用Elasticsearch

## 如何解决微服务的分布式事务问题？

2PC/XA

TCC


### 微服务时代的事务处理原则

* 1.假定网络或者服务不可靠
* 2.将全局事务建模成一-组本地ACID事务
* 3.引入事务补偿机制处理失败场景
* 4.事务始终处在一种明确的状态(不管成功还是失败)最终一致
* 6.考虑隔离性
* 7. 考虑幂等性
* 8.异步响应式， 尽量避免直接同步调用

### Saga模式

如果中间任何环节出了问题，就会触发事务的补偿流程

* 协同式(Choreography) Saga （小规模系统）
* 编排式(Orchestration) Saga （需要高可用部署）

Saga并不保证全局事务的隔离性


### 阿里分布式事务中间件Seata解析

角色

* Transaction Coordinator(TC)事务协调器
  * 维护全局事务的运行状态，驱动全局事务的提交或回滚
* TransactionManager(TM)事务管理器
  * 控制全局事务的边界，负责开启一个全局事务，并最终负责发起全局提交或全局回滚
* ResourceManager(RM)资源管理器
  * 控制分支事务，负责分支事务的注册、状态汇报，并驱动分支(本地)事务的提交和回滚

都是源于2PC/XA规范的

定制优化2PC/XA

@GlobalTransactional(timeout = 300000)

@GlobalLock 支持轻量级全局锁定隔离


AT自动模式(主要)
TCC模式
Saga模式


#### 支持框架、数据库和模式

* RPC
  * 1. Dubbo
  * 2. Spring Cloud
  * 3. Motan
  * 4. SOFA-RPC
  * 5.自定义RPC框架
* 数据库
  * MySQL、Oracle、PostgreSQL、TiDB和RDS系列等~ AT模式
* 模式
  * AT(推荐)
  * TCC
  * Saga~异步长事务

AT/TCC 要求强一:致性的同步短事务场景

https://github.com/seata/awesome-seata

### Uber微服务编排引擎Cadence解析

是一个编排引擎(orchestration engine)
面向异步长事务的场景(asynchronous long-running)
Fault-Tolerant Stateful Code Platform ( 容错有状态的编程平台)


#### Cadence编程模式

* Activity
  * 活动or任务Task or处理器Handler or微服务or Actor or分支事务执行者
* Workflow
  * 工作流or编排流程or分布式事务流程
  * 长短事务都支持
* Starter ~工作流启动/触发
  * 启动/发信号/查询
  * 同步/异步都支持

#### Cadence Activity功能

* 支持运行任何应用程序代码
* 支持是长期运行任务(heartbeating)
* 支持异步执行
* 根据设置的重试策略进行自动重试
* 支持路由到指定的主机或者进程
* 通过队列派遣任务执行
* 支持对worker进行限流
* 支持对queue进行限流

#### Cadence Workflows功能

* 支持虚拟对象技术(Virtual Objects)
* 支持事务
* 对活动(Activities)进行编排
* 支持接收外部事件并作出响应
* 有状态(包含局部变量和栈)
* 支持长期运行
* 支持持久化的时钟

#### Cadence客户端库

* Java&Go
* 无状态
* 屏蔽开发复杂性
* 支持以容错Actor方式开发分布式业务逻辑


### 拆分迁移

* 接口收口
* 双写迁移DB
  * 增量可回滚
  * 数据补偿+比对
* 数据分发去Join
* 计划+梳理+沟通+监控!!!

## 总结

### 如何解决微服务的数据一致性分发问题?

* 事务性发件箱模式(Transactional Outbox)
* 变更数据捕获(Change Data Capture, CDC)
* 双写?
  * 需要后台校验补偿
* 确保单一真实数据源(Single Source of Truth)

### 如何解决微服务的数据聚合Join问题?

* BFF聚合层
* 反正规化+流聚合
  * 分布式物化视图(Materialized View)
  *  CQRS模式
* CQRS最终一致性
  * UI更新

### 如何解决微服务的分布式事务问题?

* 2PC/XA/TCC
* Saga模式
  * 协同VS编排
  * 需考虑隔离性
* 主流开源产品
  * 2PC/XA/TCC方案~ Seata
  * 工作流/编排/Saga方案~ Cadence

### 如何实现遗留系统的解耦拆分?

* 接口收口
* 拆分迁移DB
  * 双写+开关,增量可回滚
  * 数据补偿+比对
* 数据分发去Join

## SessionServer项目背景

### SessionServer

粘性会话的问题

* LB负载相对不均衡
* 应用和负载均衡设备耦合
  * 一道架构的枷锁，任何调整都要考虑到它
* 应用有状态
  * 单点问题,
  * 发布问题
  * 难以水平扩展
    * Scale Out
    * 跨数据中心HA
    
四种常用会话技术

* 1.粘性会话
* 2.纯客户端会话（cookie 网络开销、安全风险）
* 3.服务器共享会话（同步开销大）
* 4.服务器端集中式会话

SessionServer架构和设计目标

* 1.消除Sticky Session, 支持应用Scale Out
* 2.高并发
* 3.高性能(99.99% < 10ms)
* 4.高可用(HA)
* 5.水平按需扩展
  * 透明扩容
* 6.透明升级
* 7.支持跨数据中心
* 8.接入简单(不用改代码)
* 9.监控和运维友好

细粒度的监控数据，是后续性能调优还有容量规划的依据

## SaaS项目healthchecks.io的背景和架构

* 定时任务(Cron Job)监控
* 开源SaaS项目
  * Python & Django
* 一个人的SaaS产品

## 如何设计一个分布式限流系统？

令牌桶or漏桶算法

## 微服务

|代 |代表开源产品| 年代|集成/部署模式|优势|不足|支持组织|
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|1|Dubbo/Finagle|2012年前|框架集成|性能，代码层控制粒度|多语言成本高，业务代码耦合，升级迁移麻烦|Apache +阿里|
|1.5|Airbnb SmartStack|2013|边车/主机独立进程|支持多语言， 非侵入|部署运维成本，非标准|Airbnb|
|2|Spring Cloud|2013年后|框架组件|性能，代码层控制粒度，组件化|多语言成本高，业务代码耦合，升级迁移麻烦|Pivotal/NetfliX|
|3|Istio/ Envoy|2016年后|边车/服务网格|支持多语言，非侵入，标准化，和k8s无缝集成|无法控制到业务代码层|谷歌/IBM/Lyft/RedHat/Cisco|


K8s + ServiceMesh是微服务的未来

数据中心虚拟化+跨横切面关注点下沉，是行业总趋势

## EnvoyProxy 反向代理

