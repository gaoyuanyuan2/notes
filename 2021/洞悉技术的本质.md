# 极客学习 

* 基础课程，操作系统、计算机网络、编译原理等等，当然还有数据结构和算法。

* 通过在公司工作提高自己的技能，让自己可以更为独立和自由地生活。

* 提高代码的重用度，能自动化的就自动化，和需求人员谈需求，简化掉需求。

* 并不是社会不尊重程序员，只要你能帮上大忙，就一定会赢得别人的尊重。

## 安全

* 1.使用开源的框架必须实时关注其动态，特别是安全漏洞方面

* 2.任何公开的入口，都必须进行严格的安全检查

* 3.框架的选型十分重要，必须将安全考察进去


### 以这些公司为例，我们来看看这些攻击是怎样实现的。

1. 利用程序框架或库的已知漏洞。比如这次 Equifax 被攻击，就是通过 Apache Struts 的已知漏洞。RSA 被攻击，也利用了 Adobe Flash 的已知漏洞。还有之前的“心脏流血”也是使用了 OpenSSL 的漏洞……

2. 暴力破解密码。利用密码字典库或是已经泄露的密码来“撞库”。

3. 代码注入。通过程序员代码的安全性问题，如 SQL 注入、XSS 攻击、CSRF 攻击等取得用户的权限。

4. 利用程序日志不小心泄露的信息。携程的信息泄露就是本不应该能被读取的日志没有权限保护被读到了。

5. 社会工程学。RSA 被攻击，第一道防线是人——RSA 的员工。只有员工的安全意识增强了，才能抵御此类攻击。其它的如钓鱼攻击也属于此类。

然后，除了表面的攻击之外，窃取到的信息也显示了一些数据管理上的问题。

1. 只有一层安全。Equifax 只是被黑客攻破了管理面板和数据库，就造成了数据泄露。显然这样只有一层安全防护是不够的。

2. 弱密码。Equifax 数据泄露事件绝对是管理问题。至少，密码系统应该不能让用户设置如此简单的密码，而且还要定期更换。最好的方式是通过数据证书、VPN、双因子验证的方式来登录。

3. 向公网暴露了内部系统。在公司网络管理上出现了非常严重的问题。

4. 对系统及时打安全补丁。监控业内的安全漏洞事件，及时做出响应，这是任何一个有高价值数据的公司都需要干的事。

5. 安全日志被暴露。安全日志往往包含大量信息，被暴露是非常危险的。携程的 CVV 泄露就是从日志中被读到的。

6. 保存了不必要保存的用户数据。携程保存了用户的信用卡号、有效期、姓名和 CVV 码，这些信息足以让人在网上盗刷信用卡。其实对于临时支付来说，这些信息完全可以不保存在磁盘上，临时在内存中处理完毕立即销毁，是最安全的做法。即便是快捷支付，也没有必要保存 CVV 码。安全日志也没有必要将所有信息都保存下来，比如可以只保存卡号后四位，也同样可以用于处理程序故障。

7. 密码没有被合理地散列。以现代的安全观念来说，以明文方式保存密码是很不专业的做法。进一步的是只保存密码的散列值（用安全散列算法），LinkedIn 就是这样做的。但是，散列一则需要用目前公认安全的算法（比如 SHA-2 256），而已知被攻破的算法则最好不要使用（如 MD5，能人为找到碰撞，对密码验证来说问题不大），二则要加一个安全随机数作为盐（salt）。LinkedIn 的问题正在于没有加盐，导致密码可以通过预先计算的彩虹表（rainbow table）反查出明文。这些密码明文可以用来做什么事，就不好说了，撞库什么的都有可能了。对用户来说，最好是不同网站用不同密码。

### 5 条最佳实践。

1. 理解你的软件产品中使用了哪些支持性框架和库，它们的版本号分别是多少。时刻跟踪影响这些产品和版本的最新安全性声明。

2. 建立一个流程，来快速地部署带有安全补丁的软件产品发布版，这样一旦需要因为安全方面的原因而更新支持性框架或库，就可以快速地发布。最好能在几个小时或几天内完成，而不是几周或几个月。我们发现，绝大多数被攻破的情况是因为几个月或几年都没有更新有漏洞的软件组件而引起的。

3. 所有复杂的软件都有漏洞。不要基于“支持性软件产品没有安全性漏洞”这样的假设来建立安全策略。

4. 建立多个安全层。在一个面向公网的表示层（比如 Apache Struts 框架）后面建立多级有安全防护的层次，是一种良好的软件工程实践。就算表示层被攻破，也不会直接提供出重要（或所有）后台信息资源的访问权。

5. 针对公网资源，建立对异常访问模式的监控机制。现在有很多侦测这些行为模式的开源和商业化产品，一旦发现异常访问就能发出警报。作为一种良好的运维实践，我们建议针对关键业务的网页服务应用一定要有这些监控机制。

### 技术上的安全做法

首先，我们需要把我们的关键数据定义出来，然后把这些关键数据隔离出来，隔离到一个安全级别非常高的地方。所谓安全级别非常高的地方，即这个地方需要有各种如安全审计、安全监控、安全访问的区域。

一般来说，在这个区域内，这些敏感数据只入不出。通过提供服务接口来让别的系统只能在这个区域内操作这些数据，而不是把数据传出去，让别的系统在外部来操作这些数据。举个例子，用户的手机号是敏感信息。如果有外部系统需要使用手机号，一般来说是想发个短信，那么我们这个掌管手机号数据的系统就对外提供发短信的功能，而外部系统通过UID 或是别的抽像字段来调用这个系统的发短信的 API。信用卡也一样，提供信用卡的扣款 API 而不是把卡号返回给外部系统。

另外，如果业务必需返回用户的数据，一般来说，最终用户可能需要读取自己的数据，那么，对于像信用卡这样的关键数据是死也不能返回全部数据的，只能返回一个被“马赛克”了的数据（隐藏掉部分信息）。就算需要返回一些数据（如用户的地址），那么也需要在传输层上加密返回。

而用户加密的算法一定要采用非对称加密的方式，而且还要加上密钥的自动化更换，比如：在外部系统调用 100 次或是第一个小时后就自动更换加密的密钥。这样，整个系统在运行时就完全是自动化的了，而就算黑客得到了密钥，密匙也会过期，这样可以控制泄露范围。通过上述手段，我们可以把数据控制在一个比较小的区域内。

而在这个区域内，我们依然会有相关的内部员工可以访问，因此，这个区域中的数据也是需要加密存放的，而加密使用的密钥则需要放在另外一个区域中。也就是说，被加密的数据和用于加密的密钥是由不同的人来管理的，有密钥的人没有数据，有数据的人没有密钥，这两拨人可以有访问自己系统的权限，但是没有访问对方系统的权限。这样可以让这两拨人互相审计，互相牵制，从而提高数据的安全性。比如，这两拨人是不同公司的。

而密钥一定要做到随机生成，最好是对于不同用户的数据有不同的密钥，并且时不时地就能自动化更新一下，这样就可以做到内部防范。注明一下，按道理来说，用户自己的私钥应该由用户自己来保管，而公司的系统是不存的。而用户需要更新密钥时，需要对用户做身份鉴别，可以通过双因子认证，也可以通过更为严格的物理身份验证。例如，到银行柜台拿身份证重置密码。

最后，每当这些关键信息传到外部系统，需要做通知，最好是通知用户和自己的管理员。并且限制外部系统的数据访问量，超过访问量后，需要报警或是拒绝访问。

## 何为技术领导力？

尊重技术的公司和不尊重技术的公司在初期可能还不能显现，而长期来看，差距就很明显了。

* 能够发现问题。能够发现现有方案的问题。
* 能够提供解决问题的思路和方案，并能比较这些方案的优缺点。
* 能够做出正确的技术决定。用什么样的技术、什么解决方案、怎样实现来完成一个项目。
* 能够用更优雅，更简单，更容易的方式来解决问题。
* 能够提高代码或软件的扩展性、重用性和可维护性。
* 能够用正确的方式管理团队。
  * 让正确的人做正确的事，并发挥每个人的潜力；
  * 可以提高团队的生产力和人效，找到最有价值的需求，用最少的成本实现之。并且，可以不断地提高自身和团队的标准。创新能力。
* 能够使用新的方法新的方式解决问题，追逐新的工具和技术。

##  如何才能拥有技术领导力？

* 第一，你要吃透基础技术。基础技术是各种上层技术共同的基础。

### 编程部分

* C 语言：相对于很多其他高级语言来说，C 语言更接近底层。在具备跨平台能力的前提下，它可以比较容易地被人工翻译成相应的汇编代码。它的内存管理更为直接，可以让我们直接和内存地址打交道。
* 编程范式：各种编程语言都有它们各自的编程范式，用于解决各种问题。比如面向对象编程（C++、Java）、泛型编程（C++、Go、C#）、函数式编程（JavaScript、 Python、Lisp、Haskell、Erlang）等。
* 算法和数据结构：算法（及其相应的数据结构）是程序设计的有力支撑。适当地应用算法，可以有效地抽象问题，提高程序的合理性和执行效率。算法是编程中最最重要的东西，也是计算机科学中最重要的基础。

### 系统部分

* 计算机系统原理：CPU 的体系结构（指令集 [CISC/RISC]、分支预测、缓存结构、 总线、DMA、中断、陷阱、多任务、虚拟内存、虚拟化等），内存的原理与性能特点（SRAM、DRAM、DDR-SDRAM等），磁盘的原理（机械硬盘 [盘面、磁头臂、磁头、启停区、寻道等]、固态硬盘 [页映射、块的合并与回收算法、TRIM 指令等]），GPU 的原理等。
 
* 操作系统原理和基础：进程、进程管理、线程、线程调度、多核的缓存一致性、信号量、物理内存管理、虚拟内存管理、内存分配、文件系统、磁盘管理等。

* 网络基础：计算机网络是现代计算机不可或缺的一部分。需要了解基本的网络层次结构（ISO/OSI 模型、TCP/IP 协议栈），包括物理层、数据链路层（包含错误重发机制）、网络层（包含路由机制）、传输层（包含连接保持机制）、会话层、表示层、应用层（在 TCP/IP 协议栈里，这三层可以并为一层）。
 
* 数据库原理：数据库管理系统是管理数据库的利器。

* 分布式技术架构：数据库和应用程序服务器在应对互联网上数以亿计的访问量的时候，需要能进行横向扩展，这样才能提供足够高的性能。为了做到这一点，要学习分布式技术架构，包括负载均衡、DNS 解析、多子域名、无状态应用层、缓存层、数据库分片、容错和恢复机制、Paxos、Map/Reduce 操作、分布式 SQL 数据库一致性（以 Google Cloud Spanner 为代表）等知识点。

### 坚持做正确的事。做正确的事，比用正确的方式做事更重要，因为这样才始终会向目的地靠拢。哪些是正确的事呢？下面是我的观点：

* 提高效率的事。你要学习和掌握良好的时间管理方式，管理好自己的时间，能显著提高自己的效率。

* 自动化的事。程序员要充分利用自己的职业特质，当看见有可以自动化的步骤时，编写程序来自动化操作，可以显著提高效率。

* 掌握前沿技术的事。掌握前沿的技术，有利于拓展自己的眼界，也有利于找到更好的工作。需要注意的是，有些技术虽然当下很火，但未必前沿，而是因为它比较易学易用，或者性价比高。由于学习一门技术需要花费不少时间，你应该选择自己最感兴趣的，有的放矢地去学习。

* 知识密集型的事。知识密集型是相对于劳动密集型来说的。基本上，劳动密集型的事都能通过程序和机器来完成，而知识密集型的事却仍需要人来完成，所以人的价值此时就显现出来了。虽然现在人工智能似乎能做一些知识密集型的事（包括下围棋的 AlphaGo），但是在开放领域中相对于人的智能来说还是相去甚远。掌握了领域知识的人的价值依然很高。

* 技术驱动的事。不仅是指用程序驱动的事，而且还包括一切技术改变生活的事。比如自动驾驶、火星登陆等。就算自己一时用不着，你也要了解这些，以便将来这些技术来临时能适应它们。

## PaaS 层的技术主要能解决下面这些问题

* 软件生产线的问题。持续集成和持续发布，以及 DevOps 中的技术必须通过 PaaS。

* 分布式服务化的问题。分布式服务化的服务高可用、服务编排、服务调度、服务发现、服务路由，以及分布式服务化的支撑技术完全是 PaaS 的菜。

* 提高服务的可用性 SLA。提高服务可用性 SLA 所需要的分布式、高可用的技术架构和运维工具，也是 PaaS 层提供的。

* 软件能力的复用。软件工程中的核心就是软件能力的复用，这一点也完美地体现在 PaaS 平台的技术上。

Go 语言和 Docker 作为 PaaS 平台的关键技术前途是无限的

## 异常

### 你可能会觉得异常捕捉对程序的性能是有影响的，这句话也对也不对。原因是这样的。

* 异常捕捉的确是对性能有影响的，那是因为一旦异常被抛出，函数也就跟着 return 了。而程序在执行时需要处理函数栈的上下文，这会导致性能变得很慢，尤其是函数栈比较深的时候。

* 但从另一方面来说，异常的抛出基本上表明程序的错误。程序在绝大多数情况下，应该是在没有异常的情况下运行的，所以，有异常的情况应该是少数的情况，不会影响正常处理的性能问题。

### 错误返回码 vs 异常捕捉

* 对于我们并不期望会发生的事，我们可以使用异常捕捉；

* 对于我们觉得可能会发生的事，使用返回码。

### 异步编程

对此，在异步编程的世界里，我们也会有好几种处理错误的方法，最常用的就是 callback 方式。
在做异步请求的时候，注册几个 OnSuccess()、 OnFailure() 这样的函数，让在另一个线程中运行的异步代码来回调过来。

### 错误处理的最佳实践

* 统一分类的错误字典。无论你是使用错误码还是异常捕捉，都需要认真并统一地做好错误的分类。最好是在一个地方定义相关的错误。比如，HTTP 的 4XX 表示客户端有问题，5XX 则表示服务端有问题。也就是说，你要建立一个错误字典。

* 同类错误的定义最好是可以扩展的。这一点非常重要，而对于这一点，通过面向对象的继承或是像 Go 语言那样的接口多态可以很好地做到。这样可以方便地重用已有的代码。

* 定义错误的严重程度。比如，Fatal 表示重大错误，Error 表示资源或需求得不到满足，Warning 表示并不一定是个错误但还是需要引起注意，Info 表示不是错误只是一个信息，Debug 表示这是给内部开发人员用于调试程序的。

* 错误日志的输出最好使用错误码，而不是错误信息。打印错误日志的时候，应该使用统一的格式。但最好不要用错误信息，而应使用相应的错误码，错误码不一定是数字，也可以是一个能从错误字典里找到的一个唯一的可以让人读懂的关键字。这样，会非常有利于日志分析软件进行自动化监控，而不是要从错误信息中做语义分析。比如：HTTP 的日志中就会有 HTTP 的返回码，如：404。但我更推荐使用像PageNotFound这样的标识，这样人和机器都很容易处理。

* 忽略错误最好有日志。不然会给维护带来很大的麻烦。

* 对于同一个地方不停的报错，最好不要都打到日志里。不然这样会导致其它日志被淹没了，也会导致日志文件太大。最好的实践是，打出一个错误以及出现的次数。

* 不要用错误处理逻辑来处理业务逻辑。也就是说，不要使用异常捕捉这样的方式来处理业务逻辑，而是应该用条件判断。如果一个逻辑控制可以用 if - else 清楚地表达，那就不建议使用异常方式处理。异常捕捉是用来处理不期望发生的事情，而错误码则用来处理可能会发生的事。

* 对于同类的错误处理，用一样的模式。比如，对于null对象的错误，要么都用返回 null，加上条件检查的模式，要么都用抛 NullPointerException 的方式处理。不要混用，这样有助于代码规范。

* 尽可能在错误发生的地方处理错误。因为这样会让调用者变得更简单。

* 向上尽可能地返回原始的错误。如果一定要把错误返回到更高层去处理，那么，应该返回原始的错误，而不是重新发明一个错误。

* 处理错误时，总是要清理已分配的资源。这点非常关键，使用 RAII 技术，或是 try-catch-finally，或是 Go 的 defer 都可以容易地做到。

* 不推荐在循环体里处理错误。这里说的是 try-catch，绝大多数的情况你不需要这样做。最好把整个循环体外放在 try 语句块内，而在外面做 catch。

* 不要把大量的代码都放在一个 try 语句块内。一个 try 语句块内的语句应该是完成一个简单单一的事情。

* 为你的错误定义提供清楚的文档以及每种错误的代码示例。如果你是做 RESTful API 方面的，使用 Swagger 会帮你很容易搞定这个事。

* 对于异步的方式，推荐使用 Promise 模式处理错误。对于这一点，JavaScript 中有很好的实践。

* 对于分布式的系统，推荐使用 APM 相关的软件。尤其是使用 Zipkin 这样的服务调用跟踪的分析来关联错误。

## 故障处理最佳实践

### 故障源团队通常会有以下几种手段来恢复系统。

* 重启和限流。重启和限流主要解决的是可用性的问题，不是功能性的问题。重启还好说，但是限流这个事就需要相关的流控中间件了。

* 回滚操作。回滚操作一般来说是解决新代码的 bug，把代码回滚到之前的版本是快速的方式。

* 降级操作。并不是所有的代码变更都是能够回滚的，如果无法回滚，就需要降级功能了。也就是说，需要挂一个停止服务的故障公告，主要是不要把事态扩大。

* 紧急更新。紧急更新是常用的手段，这个需要强大的自动化系统，尤其是自动化测试和自动化发布系统。假如你要紧急更新 1000 多台服务器，没有一个强大的自动化发布系统是很难做到的。

### 故障前的准备工作

* 以用户功能为索引的服务和资源的全视图。首先，我们需要一个系统来记录前端用户操作界面和后端服务，以及服务使用到的硬件资源之间的关联关系。这个系统有点像 CMDB（配置管理数据库），但是比 CMDB 要大得多，是以用户端的功能来做索引的。然后，把后端的服务、服务的调用关系，以及服务使用到的资源都关联起来做成一个视图。

* 这个视图最好是由相应的自动化监控系统生成。有了这个资源图后，我们就可以很容易地找到处理故障的路径了。这就好像一张地图，如果没有地图，我们只能像个无头苍蝇一样乱试了。

* 为地图中的各个服务制定关键指标，以及一套运维流程和工具，包括应急方案。以用户功能为索引，为每个用户功能的服务都制定一个服务故障的检测、处理和恢复手册，以及相关的检测、查错或是恢复的运维工具。对于基础层和一些通用的中间件，也需要有相应的最佳实践的方法。

比如 Redis，怎样检查其是否存在问题，怎样查看其健康和运行状态？哪些是关键指标，面对常见的故障应该怎么应对，服务不可用的服务方案是什么，服务需要回滚了应该怎么操作，等等。这就好像一个导航仪，能够告诉你怎么做。而没有导航仪，就没有章法，会导致混乱。

* 设定故障的等级。还要设定不同故障等级的处理方式。比如，亚马逊一般将故障分为 4 级：1 级是全站不可用；2 级是某功能不可用，且无替代方案；3 级是某功能不可用，但有替代方案；4 级是非功能性故障，或是用户不关心的故障。阿里内的分类更多样一些，有时会根据影响多少用户来定故障等级。

* 制定故障等级，主要是为了确定该故障要牵扯进多大规模的人员来处理。故障级别越高，牵扯进来的人就越多，参与进来的管理层级别也就越高。就像亚马逊的全员上线 oncall 一样。这就好像是我们社会中常用的“红色警报”、“橙色警报”、“黄色警报”之类的，会触发不同的处理流程。

* 故障演练。故障是需要演练的。因为故障并不会时常发生，但我们又需要不断提升处理故障的能力，所以需要经常演练。一些大公司，如 Netflix，会有一个叫 Chaos Monkey 的东西，随机地在生产线上乱来。Facebook 也会有一些故障演习，比如，随机关掉线上的一些服务器。总之，要提升故障处理水平，最好的方式就是实践。见得多了，处理得多了，才能驾轻就熟。故障演练是一个非常好的实践。

* 灰度发布系统。要减少线上故障的影响范围，通过灰度发布系统来发布是一个很不错的方式。毕竟，我们在测试环境中很难模拟出线上环境的所有情况，所以，在生产线上进行灰度发布或是 A/B 测试是一件很好的事。

### 故障整改方法

* 第一，优化故障获知和故障定位的时间。

从故障发生到我们知道的时间是否可以优化得更短？
定位故障的时间是否可以更短？
有哪些地方可以做到自动化？

* 第二，优化故障的处理方式。

故障处理时的判断和章法是否科学，是否正确？
故障处理时的信息是否全透明？
故障处理时人员是否安排得当？

* 第三，优化开发过程中的问题。

Code Review 和测试中的问题和优化点。
软件架构和设计是否可以更好？
对于技术欠债或是相关的隐患问题是否被记录下来，是否有风险计划？

* 第四，优化团队能力。

如何提高团队的技术能力？
如何让团队有严谨的工程意识？

## 分布式系统架构

* 增大系统容量。
* 加强系统可用。

### 优势

* 因为模块化，所以系统模块重用度更高；

* 因为软件服务模块被拆分，开发和发布速度可以并行而变得更快；

* 系统扩展性更高；

* 团队协作流程也会得到改善；

### 比较了单体应用和分布式架构的优缺点

||传统单体架构| 分布式服务化架构|
|---|---|---|
|新功能开发|需要时间|容易开发和实现|
|部署|不经常且容易部署|经常发布，部署复杂|
|隔离性|故障影响范围大|故障影响范围小|
|架构设计|难度小|难度级数增加|
|系统性能|响应时间快，吞吐量小|响应时间慢，吞吐量大|
|系统运维|运维简单|运维复杂|
|新人上手|学习曲线大(应用逻辑)|学习曲线大(架构逻辑)|
|技术|技术单一且封闭|技术多样且开放|
|测试和查错|简单|复杂|
|系统扩展性|扩展性很差|扩展性很好|
|系统管理|重点在于开发成本|重点在于服务治理和调度|

### 存在一些问题。

* 架构设计变得复杂（尤其是其中的分布式事务）。

* 部署单个服务会比较快，但是如果一次部署需要多个服务，流程会变得复杂。

* 系统的吞吐量会变大，但是响应时间会变长。

* 运维复杂度会因为服务变多而变得很复杂。

* 架构复杂导致学习曲线变大。

* 测试和查错的复杂度增大。

* 技术多元化，这会带来维护和运维的复杂度。

* 管理分布式系统中的服务和调度变得困难和复杂。

微服务的出现使得开发速度变得更快，部署快，隔离性高，系统的扩展度也很好，但是在集成测试、运维和服务管理等方面就比较麻烦了。所以，需要一套比较好的微服务 PaaS 平台。就像 Spring Cloud 一样需要提供各种配置服务、服务发现、智能路由、控制总线……还有像 Kubernetes 提供的各式各样的部署和调度方式。

### 服务的依赖也会带来一些问题。

* 如果非关键业务被关键业务所依赖，会导致非关键业务变成一个关键业务。

* 服务依赖链中，出现“木桶短板效应”——整个 SLA 由最差的那个服务所决定。

### 系统分成四层：基础层、平台层、应用层和接入层。

* 基础层就是我们的机器、网络和存储设备等。

* 平台层就是我们的中间件层，Tomcat、MySQL、Redis、Kafka 之类的软件。

* 应用层就是我们的业务软件，比如，各种功能的服务。

* 接入层就是接入用户请求的网关、负载均衡或是 CDN、DNS 这样的东西。

### 分布式系统中需要注意的问题异

* 构系统的不标准问题
  * 软件和应用不标准。
  * 通讯协议不标准。
  * 数据格式不标准。
  * 开发和运维的过程和方法不标准。 
  
* 系统架构中的服务依赖性问题
  * 如果非关键业务被关键业务所依赖，会导致非关键业务变成一个关键业务。
  * 服务依赖链中，出现“木桶短板效应”——整个 SLA 由最差的那个服务所决定。

这里需要注意的是，很多分布式架构在应用层上做到了业务隔离，然而，在数据库结点上并没有。如果一个非关键业务把数据库拖死，那么会导致全站不可用。

* 故障发生的概率更大

防火胜于救火，在设计时就要考虑如何减轻故障。如果无法避免，也要使用自动化的方式恢复故障，减少故障影响面。人管代码，代码管机器，人不管机器！

* 多层架构的运维复杂度更大

很多公司都是按技能分工的，他们按照技能把技术团队分为产品开发、中间件开发、业务运维、系统运维等子团队。这样的分工导致的结果就是大家各管一摊，很多事情完全连不在一起。整个系统会像 “多米诺骨牌”一样，一个环节出现问题，就会倒下去一大片。因为没有一个统一的运维视图，不知道一个服务调用是如何经过每一个服务和资源，也就导致在出现故障时要花大量的时间在沟通和定位问题上。


### 构建分布式系统的目的是增加系统容量，提高系统的可用性，转换成技术方面，也就是完成下面两件事

* 大流量处理。

通过集群技术把大规模并发请求的负载分散到不同的机器上。

* 关键业务保护。

提高后台服务的可用性，把故障隔离起来阻止多米诺骨牌效应（雪崩效应）。如果流量过大，需要对业务降级，以保护关键业务流转。

### 提高架构的性能

![提高架构的性能](/img/2.png)

* 缓存系统。

加入缓存系统，可以有效地提高系统的访问能力。从前端的浏览器，到网络，再到后端的服务，底层的数据库、文件系统、硬盘和 CPU，全都有缓存，这是提高快速访问能力最有效的手段。对于分布式系统下的缓存系统，需要的是一个缓存集群。这其中需要一个 Proxy 来做缓存的分片和路由。

* 负载均衡系统。

负载均衡系统是水平扩展的关键技术，它可以使用多台机器来共同分担一部分流量请求。

* 异步调用。

异步系统主要通过消息队列来对请求做排队处理，这样可以把前端的请求的峰值给“削平”了，而后端通过自己能够处理的速度来处理请求。这样可以增加系统的吞吐量，但是实时性就差很多了。同时，还会引入消息丢失的问题，所以要对消息做持久化，这会造成“有状态”的结点，从而增加了服务调度的难度。

* 数据分区和数据镜像。

数据分区是把数据按一定的方式分成多个区（比如通过地理位置），不同的数据区来分担不同区的流量。这需要一个数据路由的中间件，会导致跨库的 Join 和跨库的事务非常复杂。而数据镜像是把一个数据库镜像成多份一样的数据，这样就不需要数据路由的中间件了。你可以在任意结点上进行读写，内部会自行同步数据。然而，数据镜像中最大的问题就是数据的一致性问题。

### 提高架构的稳定性

![提高架构的稳定性](/img/3.png)

* 服务拆分。

服务拆分主要有两个目的：一是为了隔离故障，二是为了重用服务模块。但服务拆分完之后，会引入服务调用间的依赖问题。

* 服务冗余。

服务冗余是为了去除单点故障，并可以支持服务的弹性伸缩，以及故障迁移。然而，对于一些有状态的服务来说，冗余这些有状态的服务带来了更高的复杂性。其中一个是弹性伸缩时，需要考虑数据的复制或是重新分片，迁移的时候还要迁移数据到其它机器上。

* 限流降级。

当系统实在扛不住压力时，只能通过限流或者功能降级的方式来停掉一部分服务，或是拒绝一部分用户，以确保整个架构不会挂掉。这些技术属于保护措施。

* 高可用架构。

通常来说高可用架构是从冗余架构的角度来保障可用性。比如，多租户隔离，灾备多活，或是数据可以在其中复制保持一致性的集群。总之，就是为了不出单点故障。

* 高可用运维。

高可用运维指的是 DevOps 中的 CI/CD（持续集成 / 持续部署）。一个良好的运维应该是一条很流畅的软件发布管线，其中做了足够的自动化测试，还可以做相应的灰度发布，以及对线上系统的自动化控制。这样，可以做到“计划内”或是“非计划内”的宕机事件的时长最短。+

### 分布式系统的关键技术

* 服务治理。

服务拆分、服务调用、服务发现、服务依赖、服务的关键度定义……服务治理的最大意义是需要把服务间的依赖关系、服务调用链，以及关键的服务给梳理出来，并对这些服务进行性能和可用性方面的管理。

* 架构软件管理。

服务之间有依赖，而且有兼容性问题，所以，整体服务所形成的架构需要有架构版本管理、整体架构的生命周期管理，以及对服务的编排、聚合、事务处理等服务调度功能。

* DevOps。

分布式系统可以更为快速地更新服务，但是对于服务的测试和部署都会是挑战。所以，还需要 DevOps 的全流程，其中包括环境构建、持续集成、持续部署等。

* 自动化运维。

有了 DevOps 后，我们就可以对服务进行自动伸缩、故障迁移、配置管理、状态管理等一系列的自动化运维技术了。

* 资源调度管理。

应用层的自动化运维需要基础层的调度支持，也就是云计算 IaaS 层的计算、存储、网络等资源调度、隔离和管理。

* 整体架构监控。

如果没有一个好的监控系统，那么自动化运维和资源调度管理只可能成为一个泡影，因为监控系统是你的眼睛。没有眼睛，没有数据，就无法进行高效的运维。所以说，监控是非常重要的部分。这里的监控需要对三层系统（应用层、中间件层、基础层）进行监控。

* 流量控制。

最后是我们的流量控制，负载均衡、服务路由、熔断、降级、限流等和流量相关的调度都会在这里，包括灰度发布之类的功能也在这里。

### 分布式系统有五个关键技术

* 全栈系统监控；
* 服务 / 资源调度；
* 流量调度；
* 状态 / 数据调度；
* 开发和运维的自动化。

![分布式系统有五个关键技术](/img/1.png)

### 多层体系的监控

![多层体系的监控](/img/4.png)

### 服务治理上的一些关键技术

* 服务关键程度
* 服务依赖关系
* 服务发现
* 整个架构的版本管理
* 服务应用生命周期全管理

### 服务关键程度和服务的依赖关系

* 微服务是服务依赖最优解的上限，而服务依赖的下限是千万不要有依赖环

### 流量调度的主要功能

* 依据系统运行的情况，自动地进行流量调度，在无需人工干预的情况下，提升整个系统的稳定性；
  
* 让系统应对爆品等突发事件时，在弹性计算扩缩容的较长时间窗口内或底层资源消耗殆尽的情况下，保护系统平稳运行。

### 流量调度系统还可以完成以下几方面的事情(所有的这些都应该是一个 API Gateway 应该做的事)

* 服务流控。服务发现、服务路由、服务降级、服务熔断、服务保护等。
* 流量控制。负载均衡、流量分配、流量控制、异地灾备（多活）等。
* 流量管理。协议转换、请求校验、数据缓存、数据计算等。

### 流量调度的关键技术

* 高性能。

API Gateway 必须使用高性能的技术，所以，也就需要使用高性能的语言。

* 扛流量。

要能扛流量，就需要使用集群技术。集群技术的关键点是在集群内的各个结点中共享数据。这就需要使用像 Paxos、Raft、Gossip 这样的通讯协议。因为 Gateway 需要部署在广域网上，所以还需要集群的分组技术。

* 业务逻辑。

API Gateway 需要有简单的业务逻辑，所以，最好是像 AWS 的 Lambda 服务一样，可以让人注入不同语言的简单业务逻辑。

* 服务化。

一个好的 API Gateway 需要能够通过 Admin API 来不停机地管理配置变更，而不是通过一个.conf 文件来人肉地修改配置。

### 在解决数据副本间的一致性问题时，我们有一些技术方案。

* Master-Slave 方案。
* Master-Master 方案。
* 两阶段和三阶段提交方案。
* Paxos 方案。

### PaaS 平台的本质

![PaaS](/img/5.png)

一个好的 PaaS 平台应该具有分布式、服务化、自动化部署、高可用、敏捷以及分层开放的特征，并可与 IaaS 实现良好的联动。

* PaaS 调度层 – 主要是 PaaS 的自动化和分布式对于高可用高性能的管理。
* PaaS 能力服务层 – 主要是 PaaS 真正提供给用户的服务和能力。
* PaaS 的流量调度 – 主要是与流量调度相关的东西，包括对高并发的管理。
* PaaS 的运营管理 – 软件资源库、软件接入、认证和开放平台门户。
* PaaS 的运维管理 – 主要是 DevOps 相关的东西。

## CAP 定理

* 一致性（Consistency）：每次读取要么获得最近写入的数据，要么获得一个错误。
* 可用性（Availability）：每次请求都能获得一个（非错误）响应，但不保证返回的是最新写入的数据。
* 分区容忍（Partition tolerance）：尽管任意数量的消息被节点间的网络丢失（或延迟），系统仍继续运行。


### 严重的问题

* 1. 网络是稳定的。
* 2. 网络传输的延迟是零。
* 3. 网络的带宽是无穷大。
* 4. 网络是安全的。
* 5. 网络的拓扑不会改变。
* 6. 只有一个系统管理员。
* 7. 传输数据的成本为零。
* 8. 整个网络是同构的。

##  编程范式

基础性和本质性的话题，却是非常非常重要的。

范式即模式、方法，是一类典型的编程风格，是指从事软件工程的一类典型的风格。

数据类型就好像螺帽一样，有多种接口方式：平口的、有十字的、有六角的等，而螺丝刀就像是函数，或是用来操作这些螺丝的算法或代码。

* C 语言把非常底层的控制权交给了程序员，它设计的理念是：
  * 相信程序员；
  * 不会阻止程序员做任何底层的事；
  * 保持语言的最小和最简的特性；
  * 保证 C 语言的最快的运行速度，那怕牺牲移值性。
  
C++是解决了 C 语言泛型编程的问题。

* 程序语言的类型
  * 程序语言的安全性
    * 强类型语言提供更多的安全性，但是并不能保证绝对的安全
  * 利于编译器的优化
    * 可以让编译器明确地知道程序员的意图
  * 代码的可读性
    * 有类型的编程语言，可以让代码更易读和更易维护
  * 抽象化
    * 我们使用整型或是浮点型来取代底层的字节实现，我们可以将字符串设计成一个值，而不是底层字节的数组。从高层上来说，类型可以用来定义不同模块间的交互协议，比如函数的入参类型和返回类型，从而可以让接口更有语义，而且不同的模块数据交换更为直观和易懂。
 
 
* 静态类型语言
  * C、C++、Java，一种是动态
* 类型语言
  * Python、PHP、JavaScript
  
动态类型带来的另一个问题，就是运行时识别（这个是比较耗性能的）。

类型是对内存的一种抽象。不同的类型，会有不同的内存布局和内存分配的策略。

不同的类型，有不同的操作。所以，对于特定的类型，也有特定的一组操作。

编程语言本质上帮助程序员屏蔽底层机器代码的实现，而让我们可以更为关注于业务逻辑代码。

C 语言偏向于让程序员可以控制更多的底层细节，而 Java 和 Python 则让程序员更多地关注业务功能的实现。而 C++ 则是两者都想要，导致语言在设计上非常复杂。

### 函数式编程

如何做更为抽象的泛型呢？答案就是函数式编程（Functional Programming）。

主要特点，代码简洁，易于理解，能便于进行并行执行，易于做代码重构，函数执行没有顺序上的问题，支持惰性求值，具有函数的确定性——无论在什么场景下都会得到同样的结果。

### 面向对象编程

面向对象的编程有三大特性：封装、继承和多态。

它将对象作为程序的基本单元，将程序和数据封装其中，以提高软件的可重用性、灵活性和可扩展性，对象里的程序可以访问及修改对象相关联的数据

面向对象程序设计推广了程序的灵活性和可维护性，并且在大型项目设计中广为应用。

支持者声称面向对象程序设计要比以往的做法更加便于学习，因为它能够让人们更简单地设计并维护程序，使得程序更加便于分析、设计、理解。

优点

能和真实的世界交相辉映，符合人的直觉。
面向对象和数据库模型设计类型，更多地关注对象间的模型设计。
强调于“名词”而不是“动词”，更多地关注对象和对象间的接口。
根据业务的特征形成一个个高内聚的对象，有效地分离了抽象和具体实现，增强了可重用性和可扩展性。
拥有大量非常优秀的设计原则和设计模式。
S.O.L.I.D（单一功能、开闭原则、里氏替换、接口隔离以及依赖反转，是面向对象设计的五个基本原则）、IoC/DIP……

缺点

代码都需要附着在一个类上，从一侧面上说，其鼓励了类型。
代码需要通过对象来达到抽象的效果，导致了相当厚重的“代码粘合层”。
因为太多的封装以及对状态的鼓励，导致了大量不透明并在并发下出现很多问题。

通过对象来达到抽象结果，把代码分散在不同的类里面，然后，要让它们执行起来，就需要把这些类粘合起来。所以，它另外一方面鼓励相当厚重的代码黏合层（代码黏合层就是把代码黏合到这里面）。

在 Java 里有很多注入方式，像 Spring 那些注入，鼓励黏合，导致了大量的封装，完全不知道里面在干什么事情。而且封装屏蔽了细节，具体发生啥事你还不知道。这些都是面向对象不太好的地方。

### 原型的编程范式

这种在对象里面直接修改的玩法，虽然这个特性可以带来运行时的灵活性，我们可以在运行时修改一个 prototype，给它增加甚至删除属性和方法。但是其带来了执行的不确定性，也有安全性的问题，而代码还变得不可预测，这有点黑科技的味道了。因为这些不像静态类型系统，没有一个不可变的契约对代码的确定性有保证，所以，需要使用者来自己保证。

### 编程的本质

任何算法都会有两个部分， 一个是 Logic 部分，这是用来解决实际问题的。另一个是 Control 部分，这是用来决定用什么策略来解决问题。Logic 部分是真正意义上的解决问题的算法，而 Control 部分只是影响解决这个问题的效率。程序运行的效率问题和程序的逻辑其实是没有关系的。我们认为，如果将 Logic 和 Control 部分有效地分开，那么代码就会变得更容易改进和维护。

Programs = Algorithms + Data Structures
Algorithm = Logic + Control

* Control 是可以标准化的。比如：遍历数据、查找数据、多线程、并发、异步等，都是可以标准化的。
* 因为 Control 需要处理数据，所以标准化 Control，需要标准化 Data Structure，我们可以通过泛型编程来解决这个事。
* 而 Control 还要处理用户的业务逻辑，即 Logic。所以，我们可以通过标准化接口 / 协议来实现，我们的 Control 模式可以适配于任何的 Logic。

控制一个程序流转的方式，即程序执行的方式，并行还是串行，同步还是异步，以及调度不同执行路径或模块，数据之间的存储关系，这些和业务逻辑没有关系。

DSL 的描述是“Logic”(细节)，而我们的 check_form 则成了“Control”（步骤），代码就非常好看了。


#### 这就是编程的本质

* Logic 部分才是真正有意义的（What）
* Control 部分只是影响 Logic 部分的效率（How）

人的左脑的特性是：

理性分析型
喜欢数据证据
线性思维
陷入细节
具体化的

人的右脑的特性是：

直觉型
想象力
非线性
宏观思维
抽象化的

## 弹力设计

* 容错设计又叫弹力设计
  * 容错能力（服务隔离、异步调用、请求幂等性）
  * 可伸缩性（有 / 无状态的服务）
  * 一致性（补偿事务、重试）
  * 应对大流量的能力（熔断、降级）

在确保系统正确性的前提下，系统的可用性是弹力设计保障的重点。

### 认识故障

* 无计划的
  * 系统级故障，包括主机、操作系统、中间件、数据库、网络、电源以及外围设备。
  * 数据和中介的故障，包括人员误操作、硬盘故障、数据乱了。
  * 还有自然灾害、人为破坏，以及供电问题等。
  
* 有计划的
  * 日常任务：备份，容量规划，用户和安全管理，后台批处理应用。
  * 运维相关：数据库维护、应用维护、中间件维护、操作系统维护、网络维护。
  * 升级相关：数据库、应用、中间件、操作系统、网络，包括硬件升级。
  
* 我们再给它们归个类。
  * 1. 网络问题。网络链接出现问题，网络带宽出现拥塞……
  * 2. 性能问题。数据库慢 SQL、Java Full GC、硬盘 IO 过大、CPU 飙高、内存不足……
  * 3. 安全问题。被网络攻击，如 DDoS 等。
  * 4. 运维问题。系统总是在被更新和修改，架构也在不断地被调整，监控问题……
  * 5. 管理问题。没有梳理出关键服务以及服务的依赖关系，运行信息没有和控制系统同步……
  * 6. 硬件问题。硬盘损坏、网卡出问题、交换机出问题、机房掉电、挖掘机问题……
  
* 故障是正常的，而且是常见的。
* 故障是不可预测突发的，而且相当难缠。

不要尝试着去避免故障，而是要把处理故障的代码当成正常的功能做在架构里写在代码里。

* 为什么我们把这个设计叫做弹力（Resiliency）。
  * 一方面，在好的情况下，这个事对于我们的用户和内部运维来说是完全透明的，系统自动修复不需要人的干预。
  * 另一方面，如果修复不了，系统能够做自我保护，而不让事态变糟糕。
  
### 隔离设计

* 对于系统的分离有两种方式
  * 一种是以服务的种类来做分离
  * 一种是以用户来做分离（即多租户）

在亚马逊，每个服务都有自己的一个数据库，每个数据库中都保存着和这个业务相关的数据和相应的处理状态。而每个服务从一开始就准备好了对外暴露。同时，这也是微服务所推荐的架构方式。

* 存在以下一些问题
  * 如果我们需要同时获得多个版块的数据，那么就需要调用多个服务，这会降低性能。
  * 注意，这里性能降低指的是响应时间，而不是吞吐量（相反，在这种架构下，吞吐量可以得到提高）。
  
一般来说，我们需要小心地设计用户交互，最好不要让用户在一个页面上获得所有的数据。对于目前的手机端上来说，因为手机屏幕尺寸比较小，所以，也不可能在一个屏幕页上展示太多的内容。

如果有大数据平台，就需要把这些数据都抽取到一个数据仓库中进行计算，这也增加了数据合并的复杂度。

一个版块的故障也会导致整个流程走不下去，同样会导致整体业务故障。

在业务流程上做成 Step-by-Step 的方式，这样用户交互的每一步都可以保存，以便故障恢复后可以继续执行，而不是从头执行。


#### “二阶段提交”这样的方案。

在亚马逊中，使用的是 Plan – Reserve – Commit/Cancel 模式。

也就是说，先做一个 plan 的 API 调用，然后各个子系统 reserve 住相应的资源，如果成功，则 Commit；如果有一个失败，则整体 Cancel。

这其实很像阿里的 TCC – try confirm/cancel。

#### “多租户”模式。对于一些

比较大的客户，我们可以为他们设置专门独立的服务实例，或是服务集群与其他客户隔离开来，对于一些比较小的用户来说，可以让他们共享一个服务实例，这样可以节省相关的资源。

通常来说多租户的做法有三种。
1. 完全独立的设计。每个租户有自己完全独立的服务和数据。
2. 独立的数据分区，共享的服务。多租户的服务是共享的，但数据是分开隔离的。
3. 共享的服务，共享的数据分区。每个租户的数据和服务都是共享的。

一般来说，技术方案会使用折衷方案，也就是中间方案，服务是共享的，数据通过分区来隔离，而对于一些比较重要的租户（需要好的隔离性），则使用完全独立的方式。

然而，在虚拟化技术非常成熟的今天，我们完全可以使用“完全独立”（完全隔离）的方案，通过底层的虚拟化技术（Hypervisor 的技术，如 KVM，或是 Linux Container 的技术，如 Docker）来实现物理资源的共享和成本的节约。

#### 隔离设计的重点

* 1. 我们需要定义好隔离业务的大小和粒度，过大和过小都不好。这需要认真地做业务上的需求和系统分析。
* 2. 无论是做系统版块还是多租户的隔离，你都需要考虑系统的复杂度、成本、性能、资源使用的问题，找到一个合适的均衡方案，或是分布实施的方案尤其重要，这其中需要你定义好要什么和不要什么。因为，我们不可能做出一个什么都能满足的系统。
* 3. 隔离模式需要配置一些高可用、重试、异步、消息中间件，流控、熔断等设计模式的方式配套使用。
* 4. 不要忘记了分布式系统中的运维的复杂度的提升，要能驾驭得好的话，还需要很多自动化运维的工具，尤其是使用像容器或是虚拟机这样的虚拟化技术可以帮助我们更方便地管理，和对比资源更好地利用。否则做出来了也管理不好。
* 5. 最后，你需要一个非常完整的能够看得到所有服务的监控系统，这点非常重要。

### 异步通讯设计

面对超高吐吞量的场景下，异步处理就比同步处理有比较大的优势了

* 整个同步调用链的性能会由最慢的那个服务所决定
* 同步调用会导致调用方一直在等待被调用方完成，如果一层接一层地同步调用下去，所有的参与方会有相同的等待时间。这会非常消耗调用方的资源。因为调用方需要保存现场（Context）等待远端返回，所以对于并发比较高的场景来说，这样的等待可能会极度消耗资源。
* 异步通讯相对于同步通讯来说，除了可以增加系统的吞吐量之外，最大的一个好处是其可以让服务间的解耦更为彻底，系统的调用方和被调用方可以按照自己的速率而不是步调一致，从而可以更好地保护系统，让系统更有弹力。

* 一种是发送方时不时地去轮询一下，问一下干没干完。
* 另一种方式是发送方注册一个回调方法，也就是接收方处理完后回调请求方。

* Broker 模式
  * 必须是高可用的，因为它成了整个系统的关键；
  * 必须是高性能而且是可以水平扩展的；
  * 必须是可以持久化不丢数据的。

* 事件驱动方式的好处至少有五个。
  * 服务间的依赖没有了，服务间是平等的，每个服务都是高度可重用并可被替换的。
  * 服务的开发、测试、运维，以及故障处理都是高度隔离的。
  * 服务间通过事件关联，所以服务间是不会相互 block 的。
  * 在服务间增加一些 Adapter（如日志、认证、版本、限流、降级、熔断等）相当容易。
  * 服务间的吞吐也被解开了，各个服务可以按照自己的处理速度处理。

* 事件驱动的架构也会有一些不好的地方。
  * 业务流程不再那么明显和好管理。整个架构变得比较复杂。解决这个问题需要有一些可视化的工具来呈现整体业务流程。
  * 事件可能会乱序。这会带来非常 Bug 的事。解决这个问题需要很好地管理一个状态机的控制。
  * 事务处理变得复杂。需要使用两阶段提交来做强一致性，或是退缩到最终一致性。

* 异步通讯。
  * 异步通讯最重要的是解耦服务间的依赖。最佳解耦的方式是通过 Broker 的机制。
  * 解耦的目的是让各个服务的隔离性更好，这样不会出现“一倒倒一片”的故障。
  * 异步通讯的架构可以获得更大的吞吐量，而且各个服务间的性能不受干扰相对独立。
  * 利用 Broker 或队列的方式还可以达到把抖动的吞吐量变成均匀的吞吐量，这就是所谓的“削峰”，这对后端系统是个不错的保护。
  * 服务相对独立，在部署、扩容和运维上都可以做到独立不受其他服务的干扰。

* 设计成异步通信的时候需要注意如下事宜。
  * 用于异步通讯的中间件 Broker 成为了关键，需要设计成高可用不丢消息的。另外，因为是分布式的，所以可能很难保证消息的顺序，因此你的设计最好不依赖于消息的顺序。
  * 异步通讯会导致业务处理流程不那么直观，因为像接力一样，所以在 Broker 上需要有相关的服务消息跟踪机制，否则出现问题后不容易调试。
  * 因为服务间只通过消息交互，所以业务状态最好由一个总控方来管理，这个总控方维护一个业务流程的状态变迁逻辑，以便系统发生故障后知道业务处理到了哪一步，从而可以在故障清除后继续处理。
    * 这样的设计常见于银行的对账程序，银行系统会有大量的外部系统通讯，比如跨行的交易、跨企业的交易，等等。所以，为了保证整体数据的一致性，或是避免漏处理及处理错的交易，需要有对账系统，这其实就是那个总控，这也是为什么银行有的交易是 T+1（隔天结算），就是因为要对个账，确保数据是对的。
  * 消息传递中，可能有的业务逻辑会有像 TCP 协议那样的 send 和 ACK 机制。比如：A 服务发出一个消息之后，开始等待处理方的 ACK，如果等不到的话，就需要做重传。此时，需要处理方有幂等的处理，即同一件消息无论收到多少次都只处理一次。
  
### 幂等性设计

因为系统超时，而调用户方重试一下，会给我们的系统带来不一致的副作用。

* 在这种情况下，一般有两种处理方式。
  * 一种是需要下游系统提供相应的查询接口。上游系统在 timeout 后去查询一下。如果查到了，就表明已经做了，成功了就不用做了，失败了就走失败流程。
  * 另一种是通过幂等性的方式。也就是说，把这个查询操作交给下游系统，我上游系统只管重试，下游系统保证一次和多次的请求结果是一样的。

* 全局 ID
  * 开源项目 Snowflake。它是一个分布式 ID 的生成算法。其核心思想是，产生一个 long 型的 ID

* HTTP 的幂等性
  * HTTP GET 方法用于获取资源，不应有副作用，所以是幂等的。
  * HTTP HEAD 和 GET 本质是一样的，区别在于 HEAD 不含有呈现数据，而仅仅是 HTTP 头信息，不应用有副作用，也是幂等的。
  * HTTP OPTIONS 主要用于获取当前 URL 所支持的方法，所以也是幂等的。
  * HTTP DELETE 方法用于删除资源，有副作用，但它应该满足幂等性。
  * HTTP POST 方法用于创建资源，所对应的 URI 并非创建的资源本身，而是去执行创建动作的操作者，有副作用，不满足幂等性。
  * HTTP PUT 方法用于创建或更新操作，所对应的 URI 是要创建或更新的资源本身，有副作用，它应该满足幂等性。

当然，更为稳妥的做法是，后端成功后向前端返回 302 跳转，把用户的前端页跳转到 GET 请求，把刚刚 POST 的数据给展示出来。如果是 Web 上的最好还把之前的表单设置成过期，这样用户不能通过浏览器后退按钮来重新提交。这个模式又叫做 PRG 模式（Post/Redirect/Get）。

### 服务的状态

一直以来，无状态的服务都被当作分布式服务设计的最佳实践和铁律。因为无状态的服务对于扩展性和运维实在是太方便了。没有状态的服务，可以随意地增加和减少结点，同样可以随意地搬迁。

不太重要的数据可以放到 Redis 中，重要的数据可以放到 MySQL 中，或是像 ZooKeeper/Etcd 这样的高可用的强一致性的存储中，或是分布式文件系统中。

我们为了做成无状态的服务，会导致这些服务需要耦合第三方有状态的存储服务。一方面是有依赖，另一方面也增加了网络开销，导致服务的响应时间也会变慢。

为了减少网络开销，还需要在无状态的服务中增加缓存机制。然而，下次这个用户的请求并不一定会在同一台机器，所以，这个缓存会在所有的机器上都创建，也算是一种浪费吧。

现在分布式数据库也开始将服务和存储分离，也是为了让自己的系统更有弹力。


* 有状态的服务有这些好处。
  * 数据本地化（Data Locality）。一方面状态和数据是本机保存，这方面不但有更低的延时，而且对于数据密集型的应用来说，这会更快。
  * 更高的可用性和更强的一致性。也就是 CAP 原理中的 A 和 C。

有状态的服务，它们通过 Sticky Session、一致性 Hash 和 DHT 等技术实现状态和请求的关联，并将数据同步到分布式数据库中；

当一个结点挂掉了以后，在另外一个地方重新恢复这个结点时，这个结点需要把数据同步过来才能提供服务。然而，如果数据量过大，这个过程可能会很漫长，这也会影响我们系统的可用性。

同时为了避免挂掉，还需要把数据持久化在硬盘上，这个硬盘可以是挂载到本地硬盘的一个外部分布式的文件卷。

这样当结点挂掉以后，以另外一个宿主机上启动一个新的服务实例时，这个服务可以从远程把之前的文件系统挂载过来。然后，在启动的过程中就装载好了大多数的数据，从而可以从网络其它结点上同步少量的数据，因而可以快速地恢复和提供服务。


### 补偿事务

如果一个步骤失败了，那么要么回滚到以前的服务调用，要么不断重试保证所有的步骤都成功。

如果需要强一致性，那在业务层上就需要使用“两阶段提交”这样的方式。

* BASE
  * Basic Availability：基本可用。这意味着，系统可以出现暂时不可用的状态，而后面会快速恢复。
  * Soft-state：软状态。它是我们前面的“有状态”和“无状态”的服务的一种中间状态。也就是说，为了提高性能，我们可以让服务暂时保存一些状态或数据，这些状态和数据不是强一致性的。
  * Eventual Consistency：最终一致性，系统在一个短暂的时间段内是不一致的，但最终整个系统看到的数据是一致的。


BASE 的玩法是，大家都可以同时下单，这个时候不需要去真正地分配库存，然后系统异步地处理订单，而且是批量的处理。因为下单的时候没有真正去扣减库存，所以，有可能会有超卖的情况。而后台的系统会异步地处理订单时，发现库存没有了，于是才会告诉用户你没有购买成功。

酸（ACID）强调的是一致性（CAP 中的 C），而碱（BASE）强调的是可用性（CAP 中的 A）

#### 业务补偿

对于业务补偿来说，首先需要将服务做成幂等性的，如果一个事务失败了或是超时了，我们需要不断地重试，努力地达到最终我们想要的状态。然后，如果我们不能达到这个我们想要的状态，我们需要把整个状态恢复到之前的状态。另外，如果有变化的请求，我们需要启动整个事务的业务更新机制。

* 业务补偿机制需要做到下面这几点
  * 要能清楚地描述出要达到什么样的状态，以及如果其中的条件不满足，那么，我们要回退到哪一个状态。这就是所谓的整个业务的起始状态定义。
  * 整条业务跑起来的时候，我们可以串行或并行地做这些事。总之，我们的系统需要努力地通过一系列的操作达到一个我们想要的状态。如果达不到，就需要通过补偿机制回滚到之前的状态。这就是所谓的状态拟合。
  * 对于已经完成的事务进行整体修改，可以考虑成一个修改事务。
  

* 业务补偿主要做两件事。
  * 努力地把一个业务流程执行完成。
  * 如果执行不下去，需要启动补偿机制，回滚业务流程。
  
* 几个重点。
  * 因为要把一个业务流程执行完成，需要这个流程中所涉及的服务方支持幂等性。并且在上游有重试机制。
  * 我们需要小心维护和监控整个过程的状态，所以，千万不要把这些状态放到不同的组件中，最好是一个业务流程的控制方来做这个事，也就是一个工作流引擎。所以，这个工作流引擎是需要高可用和稳定的。这就好像旅行代理机构一样，我们把需求告诉它，它会帮我们搞定所有的事。如果有问题，也会帮我们回滚和补偿的。
  * 补偿的业务逻辑和流程不一定非得是严格反向操作。有时候可以并行，有时候，可能会更简单。总之，设计业务正向流程的时候，也需要设计业务的反向补偿流程。
  * 我们要清楚地知道，业务补偿的业务逻辑是强业务相关的，很难做成通用的。
  * 下层的业务方最好提供短期的资源预留机制。就像电商中的把货品的库存预先占住等待用户在 15 分钟内支付。如果没有收到用户的支付，则释放库存。然后回滚到之前的下单操作，等待用户重新下单。

### 重试设计

设计重试时，我们需要定义出什么情况下需要重试，例如，调用超时、被调用端返回了某种可以重试的错误（如繁忙中、流控中、维护中、资源不足等）。

而对于一些别的错误，则最好不要重试，比如：业务级的错误（如没有权限、或是非法数据等错误），技术上的错误（如：HTTP 的 503 等，这种原因可能是触发了代码的 bug，重试下去没有意义）。

在重试的设计中，我们一般都会引入，Exponential Backoff 的策略，也就是所谓的 " 指数级退避 "。

#### Spring 的重试策略

Spring Retry 是一个单独实现重试功能的项目，我们可以通过 Annotation 的方式使用。

* 重试的设计重点主要如下：
  * 要确定什么样的错误下需要重试；
  * 重试的时间和重试的次数。
  * 如果超过重试次数，或是一段时间，那么重试就没有意义了。
  * 重试还需要考虑被调用方是否有幂等的设计。如果没有，那么重试是不安全的，可能会导致一个相同的操作被执行多次。
  * 重试的代码比较简单也比较通用，完全可以不用侵入到业务代码中。
  * 对于有事务相关的操作。我们可能会希望能重试成功，而不至于走业务补偿那样的复杂的回退流程。对此，我们可能需要一个比较长的时间来做重试，但是我们需要保存请求的上下文，这可能对程序的运行有比较大的开销，因此，有一些设计会先把这样的上下文暂存在本机或是数据库中，然后腾出资源来做别的事，过一会再回来把之前的请求从存储中捞出来重试。
  
### 熔断设计

使用熔断设计可以保护后端不会过载。

熔断器模式也可以使应用程序能够诊断错误是否已经修正。如果已经修正，应用程序会再次尝试调用操作。

熔断器可以使用状态机来实现，内部模拟以下几种状态。

* 闭合（Closed）状态
* 断开 (Open) 状态
* 半开（Half-Open）状态

Hystrix 会在内存中维护一个数组，其中记录着每一个周期的请求结果的统计。超过时长长度的元素会被删除掉。

* 熔断设计的重点
  * 在实现熔断器模式的时候，以下这些因素需可能需要考虑。
  * 错误的类型。需要注意的是请求失败的原因会有很多种。你需要根据不同的错误情况来调整相应的策略。所以，熔断和重试一样，需要对返回的错误进行识别。一些错误先走重试的策略（比如限流，或是超时），重试几次后再打开熔断。一些错误是远程服务挂掉，恢复时间比较长；这种错误不必走重试，就可以直接打开熔断策略。
  * 日志监控。熔断器应该能够记录所有失败的请求，以及一些可能会尝试成功的请求，使得管理员能够监控使用熔断器保护服务的执行情况。
  * 测试服务是否可用。在断开状态下，熔断器可以采用定期地 ping 一下远程服务的健康检查接口，来判断服务是否恢复，而不是使用计时器来自动切换到半开状态。这样做的一个好处是，在服务恢复的情况下，不需要真实的用户流量就可以把状态从半开状态切回关闭状态。否则在半开状态下，即便服务已恢复了，也需要用户真实的请求来恢复，这会影响用户的真实请求。
  * 手动重置。在系统中对于失败操作的恢复时间是很难确定的，提供一个手动重置功能能够使得管理员可以手动地强制将熔断器切换到闭合状态。同样的，如果受熔断器保护的服务暂时不可用的话，管理员能够强制将熔断器设置为断开状态。
  * 并发问题。相同的熔断器有可能被大量并发请求同时访问。熔断器的实现不应该阻塞并发的请求或者增加每次请求调用的负担。尤其是其中对调用结果的统计，一般来说会成为一个共享的数据结构，它会导致有锁的情况。在这种情况下，最好使用一些无锁的数据结构，或是 atomic 的原子操作。这样会带来更好的性能。
  * 资源分区。有时候，我们会把资源分布在不同的分区上。比如，数据库的分库分表，某个分区可能出现问题，而其它分区还可用。在这种情况下，单一的熔断器会把所有的分区访问给混为一谈，从而，一旦开始熔断，那么所有的分区都会受到熔断影响。或是出现一会儿熔断一会儿又好，来来回回的情况。所以，熔断器需要考虑这样的问题，只对有问题的分区进行熔断，而不是整体。
  * 重试错误的请求。有时候，错误和请求的数据和参数有关系，所以，记录下出错的请求，在半开状态下重试能够准确地知道服务是否真的恢复。当然，这需要被调用端支持幂等调用，否则会出现一个操作被执行多次的副作用。
  
### 限流设计

* 限流的策略
  * 拒绝服务。把多出来的请求拒绝掉。一般来说，好的限流系统在受到流量暴增时，会统计当前哪个客户端来的请求最多，直接拒掉这个客户端，这种行为可以把一些不正常的或者是带有恶意的高并发访问挡在门外。
  * 服务降级。关闭或是把后端服务做降级处理。这样可以让服务有足够的资源来处理更多的请求。降级有很多方式，一种是把一些不重要的服务给停掉，把 CPU、内存或是数据的资源让给更重要的功能；一种是不再返回全量数据，只返回部分数据。
  * 因为全量数据需要做 SQL Join 操作，部分的数据则不需要，所以可以让 SQL 执行更快，还有最快的一种是直接返回预设的缓存，以牺牲一致性的方式来获得更大的性能吞吐。
  * 特权请求。所谓特权请求的意思是，资源不够了，我只能把有限的资源分给重要的用户，比如：分给权利更高的 VIP 用户。在多租户系统下，限流的时候应该保大客户的，所以大客户有特权可以优先处理，而其它的非特权用户就得让路了。
  * 延时处理。在这种情况下，一般会有一个队列来缓冲大量的请求，这个队列如果满了，那么就只能拒绝用户了，如果这个队列中的任务超时了，也要返回系统繁忙的错误了。使用缓冲队列只是为了减缓压力，一般用于应对短暂的峰刺请求。
  * 弹性伸缩。动用自动化运维的方式对相应的服务做自动化的伸缩。这个需要一个应用性能的监控系统，能够感知到目前最繁忙的 TOP 5 的服务是哪几个。
    * 然后去伸缩它们，还需要一个自动化的发布、部署和服务注册的运维系统，而且还要快，越快越好。否则，系统会被压死掉了。当然，如果是数据库的压力过大，弹性伸缩应用是没什么用的，这个时候还是应该限流。

* 限流的实现方式
  * 计数器方式
  * 队列算法
    * 队列流控是以队列的的方式来处理请求。如果处理过慢，那么就会导致队列满，而开始触发限流。
  * 漏斗算法 Leaky Bucket（漏斗算法其实就是在队列请求中加上一个限流器，来让 Processor 以一个均匀的速度处理请求。）
  * 令牌桶算法 Token Bucket（主要是有一个中间人。在一个桶内按照一定的速率放入一些 token，然后，处理程序要处理请求时，需要拿到 token，才能处理；如果拿不到，则不处理。）
  * 基于响应时间的动态限流
  
设定一个特定的流控值，而是能够动态地感知系统的压力来自动化地限流。

这方面设计的典范是 TCP 协议的拥塞控制的算法。TCP 使用 RTT - Round Trip Time 来探测网络的延时和性能，从而设定相应的“滑动窗口”的大小，以让发送的速率和网络的性能相匹配。这个算法是非常精妙的，我们完全可以借鉴在我们的流控技术中。

* 限流主要是有四个目的。
  * 为了向用户承诺 SLA。我们保证我们的系统在某个速度下的响应时间以及可用性。
  * 同时，也可以用来阻止在多租户的情况下，某一用户把资源耗尽而让所有的用户都无法访问的问题。
  * 为了应对突发的流量。
  * 节约成本。我们不会为了一个不常见的尖峰来把我们的系统扩容到最大的尺寸。而是在有限的资源下能够承受比较高的流量。


* 在设计上，我们还要有以下的考量。
  * 限流应该是在架构的早期考虑。当架构形成后，限流不是很容易加入。
  * 限流模块性能必须好，而且对流量的变化也是非常灵敏的，否则太过迟钝的限流，系统早因为过载而挂掉了。
  * 限流应该有个手动的开关，这样在应急的时候，可以手动操作。
  * 当限流发生时，应该有个监控事件通知。让我们知道有限流事件发生，这样，运维人员可以及时跟进。而且还可以自动化触发扩容或降级，以缓解系统压力。
  * 当限流发生时，对于拒掉的请求，我们应该返回一个特定的限流错误码。这样，可以和其它错误区分开来。而客户端看到限流，可以调整发送速度，或是走重试机制。
  * 限流应该让后端的服务感知到。限流发生时，我们应该在协议头中塞进一个标识，比如 HTTP Header 中，放入一个限流的级别，告诉后端服务目前正在限流中。这样，后端服务可以根据这个标识决定是否做降级。
  
### 降级设计

* 降级需要牺牲掉的东西有：
  * 降低一致性。从强一致性变成最终一致性。
  * 停止次要功能。停止访问不重要的功能，从而释放出更多的资源。
  * 简化功能。把一些功能简化掉，比如，简化业务流程，或是不再返回全量数据，只返回部分数据。
  
因为降级的功能平时不会总是会发生，属于应急的情况，所以，降级的这些业务流程和功能有可能长期不用而出现 bug 或问题，对此，需要在平时做一些演练。

### 弹力设计总结

* 错误方面：调用重试 + 熔断 + 服务的幂等性设计。
* 一致性方面：强一致性使用两阶段提交、最终一致性使用异步通讯方式。
* 流控方面：使用限流 + 降级技术。
* 自动化运维方面：网关流量调度，服务监控。


* 三大块的东西。
  * 冗余服务。通过冗余服务的复本数可以消除单点故障。这需要服务发现，负载均衡，动态路由和健康检查四个功能或组件。
  * 服务解耦。通过解耦可以做到把业务隔离开来，不让服务间受影响，这样就可以有更好的稳定性。在水平层面上，需要把业务或用户分片分区（业分做隔离，用户做多租户）。在垂直层面上，需要异步通讯机制。因为应用被分解成了一个一个的服务，所以在服务的编排和聚合上，需要有工作流（像 Spring 的 Stream 或 Akka 的 flow 或是 AWS 的 Simple Workflow）来把服务给串联起来。而一致性的问题又需要业务补偿机制来做反向交易。
  * 服务容错。服务容错方面，需要有重试机制，重试机制会带来幂等操作，对于服务保护来说，熔断，限流，降级都是为了保护整个系统的稳定性，并在可用性和一致性方面在出错的情况下做一部分的妥协。

#### 弹力设计开发和运维

* 对于运维工具来说，你至少需要两个系统：
  * 一个是像 APM 这样的服务监控；
  * 另一个是服务调度的系统，如：Docker + Kubernetes。
  
## 管理设计

### 分布式锁

* 特点
  * 安全性（Safety）：在任意时刻，只有一个客户端可以获得锁（排他性）。
  * 避免死锁：客户端最终一定可以获得锁，即使锁住某个资源的客户端在释放锁之前崩溃或者网络不可达。
  * 容错性：只要锁服务集群中的大部分节点存活，Client 就可以进行加锁解锁操作。

Redis 的分布式锁服务

SET NX 命令只会在 key 不存在的时候给 key 赋值，PX 命令通知 Redis 保存这个 key 30000ms。


* 不区分 Client 会出现的一种问题。
  * Client A 获得了一个锁。
  * 当尝试释放锁的请求发送给 Redis 时被阻塞，没有及时到达 Redis。
  * 锁定时间超时，Redis 认为锁的租约到期，释放了这个锁。
  * Client B 重新申请到了这个锁。
  * Client A 的解锁请求到达，将 Client B 锁定的 key 解锁。
  * Client C 也获得了锁。
  * Client B 和 Client C 同时持有锁。

* 案例
  * 如果 Client A 先取得了锁。
  * 其它 Client（比如说 Client B）在等待 Client A 的工作完成。
  * 这个时候，如果 Client A 被挂在了某些事上，比如一个外部的阻塞调用，或是 CPU 被别的进程吃满，或是不巧碰上了 Full GC，导致 Client A 花了超过平时几倍的时间。
  * 然后，我们的锁服务因为怕死锁，就在一定时间后，把锁给释放掉了。
  * 此时，Client B 获得了锁并更新了资源。
  * 这个时候，Client A 服务缓过来了，然后也去更新了资源。于是乎，把 Client B 的更新给冲掉了。
  * 这就造成了数据出错。

要解决这个问题，你需要引入 fence（栅栏）技术。一般来说，这就是乐观锁机制，需要一个版本号排它。

* 锁服务需要有一个单调递增的版本号。
* 写数据的时候，也需要带上自己的版本号。
* 数据库服务需要保存数据的版本号，然后对请求做检查。

如果使用 ZooKeeper 做锁服务的话，那么可以使用 zxid 或 znode 的版本号来做这个 fence 版本号。

使用数据版本（Version）记录机制，即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现的。当读取数据时，将 version 字段的值一同读出，数据每更新一次，对此 version 值加一。

当我们提交更新的时候，数据库表对应记录的当前版本信息与第一次取出来的 version 值进行比对。如果数据库表当前版本号与第一次取出来的 version 值相等，则予以更新，否则认为是过期数据。

```sql
UPDATE table_name SET xxx = #{xxx}, version=version+1 where version =#{version};
```

是的，如果我们使用版本号，或是 fence token 这种方式，就不需要使用分布式锁服务了。

这种 fence token 的玩法，在数据库那边一般会用 timestamp 时间截来玩。也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则 OK，否则就是版本冲突。

* 布式锁服务的初衷和几个概念性的问题。

  * 如果获得锁的进程挂掉了怎么办？锁还不回来了，会导致死锁。一般的处理方法是在锁服务那边加上一个过期时间，如果在这个时间内锁没有被还回来，那么锁服务要自动解锁，以避免全部锁住。
  * 如果锁服务自动解锁了，新的进程就拿到锁了，但之前的进程以为自己还有锁，那么就出现了两个进程拿到了同一个锁的问题，它们在更新数据的时候就会产生问题。对于这个问题。
  * 像 Redis 那样也可以使用 Check and Set 的方式来保证数据的一致性。这就有点像计算机原子指令 CAS（Compare And Swap）一样。就是说，我在改变一个值的时候先检查一下是不是我之前读出来的值，这样来保证其间没有人改过。
  * 如果通过像 CAS 这样的操作的话，我们还需要分布式锁服务吗？的确是不需要了，不是吗？
  * 但现实生活中也有不需要更新某个数据的场景，只是为了同步或是互斥一下不同机器上的线程，这时候像 Redis 这样的分布式锁服务就有意义了。

需要分清楚：我是用来修改某个共享源的，还是用来不同进程间的同步或是互斥的。如果使用 CAS 这样的方式（无锁方式）来更新数据，那么我们是不需要使用分布式锁服务的，而后者可能是需要的。所以，这是我们在决定使用分布式锁服务前需要考虑的第一个问题——我们是否需要？

* 如果确定要分布式锁服务，你需要考虑下面几个设计。
  * 需要给一个锁被释放的方式，以避免请求者不把锁还回来，导致死锁的问题。Redis 使用超时时间，ZooKeeper 可以依靠自身的 sessionTimeout 来删除节点。
  * 分布式锁服务应该是高可用的，而且是需要持久化的。
  * 要提供非阻塞方式的锁服务。
  * 还要考虑锁的可重入性。
  
乐观加上重试机制会好一些。

### 配置中心

线程池大小、队列长度等运行参数，以及日志级别、算法策略等，还有一些是软件运行环境的参数，如 Java 的内存大小，应用启动的参数，包括操作系统的一些参数配置。

所谓动态配置其实就是软件运行时的一些配置，在运行时会被修改。比如，日志级别、降级开关、活动开关。

* 对于动态配置的管理，我们还要做好区分。一般来说，会有三个区分的维度。
  * 按运行环境分。一般来说，会有开发环境、测试环境、预发环境、生产环境。这些环境上的运行配置都不完全一样，但是理论来说，应该是大同小异的。
  * 按依赖区分。一种是依赖配置，一种是不依赖的内部配置。比如，外部依赖的 MySQL 或 Redis 的连接配置。还有一种完全是自己内部的配置。
  * 按层次分。就像云计算一样，配置也可以分成 IaaS、PaaS、SaaS 三层。基础层的配置是操作系统的配置，中间平台层的配置是中间件的配置，如 Tomcat 的配置，上层软件层的配置是应用自己的配置。

我们的配置参数中，如果有外部服务依赖的配置，强烈建议不要放在配置中心里，而要放在服务发现系统中。因为一方面这在语义上更清楚一些，另外，这样会减少因为运行不同环境而导致配置不同的差异性（如测试环境和生产环境的不同）。

这个 API 上至少需要有如下参数：服务名，配置的版本号，配置的环境。

### 边车模式

我们不需要在服务中实现控制面上的东西，如监视、日志记录、限流、熔断、服务注册、协议适配转换等这些属于控制面上的东西，而只需要专注地做好和业务逻辑相关的代码，然后，由“边车”来实现这些与业务逻辑没有关系的控制功能。

* 边车模式重点解决什么样的问题。
  * 控制和逻辑的分离。
  * 服务调用中上下文的问题。

熔断、路由、服务发现、计量、流控、监视、重试、幂等、鉴权等控制面上的功能，以及其相关的配置更新，本质来上来说，和服务的关系并不大。但是传统的工程做法是在开发层面完成这些功能，这就会导致各种维护上的问题，而且还会受到特定语言和编程框架的约束和限制。


* 一种是通过 SDK、Lib 或 Framework 软件包方式，在开发时与真实的应用服务集成起来。
* 另一种是通过像 Sidecar 这样的方式，在运维时与真实的应用服务集成起来。

* 清楚 Sidecar 适用于什么样的场景，下面罗列几个。
  * 一个比较明显的场景是对老应用系统的改造和扩展。
  * 另一个是对由多种语言混合出来的分布式服务系统进行管理和扩展。
  * 其中的应用服务由不同的供应商提供。
  * 把控制和逻辑分离，标准化控制面上的动作和技术，从而提高系统整体的稳定性和可用性。也有利于分工——并不是所有的程序员都可以做好控制面上的开发的。

* 我们还要清楚 Sidecar 不适用于什么样的场景，下面罗列几个。
  * 架构并不复杂的时候，不需要使用这个模式，直接使用 API Gateway 或者 Nginx 和 HAProxy 等即可。
  * 服务间的协议不标准且无法转换。
  * 不需要分布式的架构。
  
### 服务网格

Service Mesh 服务网格

Service Mesh 这个服务网络专注于处理服务和服务间的通讯。其主要负责构造一个稳定可靠的服务通讯的基础设施，并让整个架构更为的先进和 Cloud Native。在工程中，Service Mesh 基本来说是一组轻量级的服务代理和应用逻辑的服务在一起，并且对于应用服务是透明的。

说白了，就是下面几个特点。

* Service Mesh 是一个基础设施。
* Service Mesh 是一个轻量的服务通讯的网络代理。
* Service Mesh 对于应用服务来说是透明无侵入的。
* Service Mesh 用于解耦和分离分布式系统架构中控制层面上的东西。

### 网关模式

在谈 Service Mesh 的时候，我们提到了 Gateway。我个人觉得并不需要为每个服务的实例都配置上一个 Sidecar。其实，一个服务集群配上一个 Gateway 就可以了，或是一组类似的服务配置上一个 Gateway。

Gateway 封装内部系统的架构，并且提供 API 给各个客户端。它还可能有其他功能，如授权、监控、负载均衡、缓存、熔断、降级、限流、请求分片和管理、静态响应处理，等等。


一个网关需要有以下的功能。

* 请求路由。因为不再是 Sidecar 了，所以网关一定要有请求路由的功能。这样一来，对于调用端来说，也是一件非常方便的事情。因为调用端不需要知道自己需要用到的其它服务的地址，全部统一地交给 Gateway 来处理。
* 服务注册。为了能够代理后面的服务，并把请求路由到正确的位置上，网关应该有服务注册功能，也就是后端的服务实例可以把其提供服务的地址注册、取消注册。一般来说，注册也就是注册一些 API 接口。比如，HTTP 的 Restful 请求，可以注册相应 API 的 URI、方法、HTTP 头。 这样，Gateway 就可以根据接收到的请求中的信息来决定路由到哪一个后端的服务上。
* 负载均衡。因为一个网关可以接收多个服务实例，所以网关还需要在各个对等的服务实例上做负载均衡策略。简单点就是直接 Round-Robin 轮询，复杂点的可以设置上权重进行分发，再复杂一点还可以做到 session 粘连。
* 弹力设计。网关还可以把弹力设计中的那些异步、重试、幂等、流控、熔断、监视等都可以实现进去。这样，同样可以像 Service Mesh 那样，让应用服务只关心自己的业务逻辑（或是说数据面上的事）而不是控制逻辑（控制面）。
* 安全方面。SSL 加密及证书管理、Session 验证、授权、数据校验，以及对请求源进行恶意攻击的防范。错误处理越靠前的位置就是越好，所以，网关可以做到一个全站的接入组件来对后端的服务进行保护。

当然，网关还可以做更多更有趣的事情，比如：
* 灰度发布。网关完全可以做到对相同服务不同版本的实例进行导流，还可以收集相关的数据。这样对于软件质量的提升，甚至产品试错都有非常积极的意义。
* API 聚合。使用网关可以将多个单独请求聚合成一个请求。在微服务体系的架构中，因为服务变小了，所以一个明显的问题是，客户端可能需要多次请求才能得到所有的数据。这样一来，客户端与后端之间的频繁通信会对应用程序的性能和规模产生非常不利的影响。于是，我们可以让网关来帮客户端请求多个后端的服务（有些场景下完全可以并发请求），然后把后端服务的响应结果拼装起来，回传给客户端（当然，这个过程也可以做成异步的，但这需要客户端的配合）。
* API 编排。同样在微服务的架构下，要走完一个完整的业务流程，我们需要调用一系列 API，就像一种工作流一样，这个事完全可以通过网页来编排这个业务流程。我们可能通过一个 DSL 来定义和编排不同的 API，也可以通过像 AWS Lambda 服务那样的方式来串联不同的 API。

当 Sidecar 在架构中越来越多时，需要我们对 Sidecar 进行统一的管理。于是，我们为 Sidecar 增加了一个全局的中心控制器，就出现了我们的 Service Mesh。在中心控制器出现以后，我们发现，可以把非业务功能的东西全部实现在 Sidecar 和 Controller 中，于是就成了一个网格。业务方只需要把服务往这个网格中一放就好了，与其它服务的通讯、服务的弹力等都不用管了，像一个服务的 PaaS 平台。

然而，Service Mesh 的架构和部署太过于复杂，会让我们运维层面上的复杂度变大。为了简化这个架构的复杂度，我认为 Sidecar 的粒度应该是可粗可细的，这样更为方便。但我认为，Gateway 更为适合，而且 Gateway 只负责进入的请求，不像 Sidecar 还需要负责对外的请求。因为 Gateway 可以把一组服务给聚合起来，所以服务对外的请求可以交给对方服务的 Gateway。于是，我们只需要用一个负责进入请求的 Gateway 来简化需要同时负责进出请求的 Sidecar 的复杂度。

网关的设计重点

* 高性能
* 高可用
  * 集群化。网关要成为一个集群，其最好可以自己组成一个集群，并可以自己同步集群数据，而不需要依赖于一个第三方系统来同步数据。
  * 服务化。网关还需要做到在不间断的情况下修改配置，一种是像 Nginx reload 配置那样，可以做到不停服务，另一种是最好做到服务化。也就是说，得要有自己的 Admin API 来在运行时修改自己的配置。
  * 持续化。比如重启，就是像 Nginx 那样优雅地重启。有一个主管请求分发的主进程。当我们需要重启时，新的请求被分配到新的进程中，而老的进程处理完正在处理的请求后就退出。
* 高扩展

* 运维方面，网关应该有以下几个设计原则。
  * 业务松耦合，协议紧耦合。在业务设计上，网关不应与后面的服务之间形成服务耦合，也不应该有业务逻辑。网关应该是在网络应用层上的组件，不应该处理通讯协议体，只应该解析和处理通讯协议头。另外，除了服务发现外，网关不应该有第三方服务的依赖。
  * 应用监视，提供分析数据。网关上需要考虑应用性能的监控，除了有相应后端服务的高可用的统计之外，还需要使用 Tracing ID 实施分布式链路跟踪，并统计好一定时间内每个 API 的吞吐量、响应时间和返回码，以便启动弹力设计中的相应策略。
  * 用弹力设计保护后端服务。网关上一定要实现熔断、限流、重试和超时等弹力设计。如果一个或多个服务调用花费的时间过长，那么可接受超时并返回一部分数据，或是返回一个网关里的缓存的上一次成功请求的数据。你可以考虑一下这样的设计。
  * DevOps。因为网关这个组件太关键了，所以需要 DevOps 这样的东西，将其发生故障的概率降到最低。这个软件需要经过精良的测试，包括功能和性能的测试，还有浸泡测试。还需要有一系列自动化运维的管控工具。

* 在整体的架构方面，有如下一些注意事项。
  * 不要在网关中的代码里内置聚合后端服务的功能，而应考虑将聚合服务放在网关核心代码之外。可以使用 Plugin 的方式，也可以放在网关后面形成一个 Serverless 服务。
  * 网关应该靠近后端服务，并和后端服务使用同一个内网，这样可以保证网关和后端服务调用的低延迟，并可以减少很多网络上的问题。这里多说一句，网关处理的静态内容应该靠近用户（应该放到 CDN 上），而网关和此时的动态服务应该靠近后端服务。
  * 网关也需要做容量扩展，所以需要成为一个集群来分担前端带来的流量。这一点，要么通过 DNS 轮询的方式实现，要么通过 CDN 来做流量调度，或者通过更为底层的性能更高的负载均衡设备。
  * 对于服务发现，可以做一个时间不长的缓存，这样不需要每次请求都去查一下相关的服务所在的地方。当然，如果你的系统不复杂，可以考虑把服务发现的功能直接集成进网关中。
  * 为网关考虑 bulkhead 设计方式。用不同的网关服务不同的后端服务，或是用不同的网关服务前端不同的客户。

* 另外，因为网关是为用户请求和后端服务的桥接装置，所以需要考虑一些安全方面的事宜。具体如下：
  * 加密数据。可以把 SSL 相关的证书放到网关上，由网关做统一的 SSL 传输管理。
  * 校验用户的请求。一些基本的用户验证可以放在网关上来做，比如用户是否已登录，用户请求中的 token 是否合法等。但是，我们需要权衡一下，网关是否需要校验用户的输入。因为这样一来，网关就需要从只关心协议头，到需要关心协议体。而协议体中的东西一方面不像协议头是标准的，另一方面解析协议体还要耗费大量的运行时间，从而降低网关的性能。对此，我想说的是，看具体需求，一方面如果协议体是标准的，那么可以干；另一方面，对于解析协议所带来的性能问题，需要做相应的隔离。
  * 检测异常访问。网关需要检测一些异常访问，比如，在一段比较短的时间内请求次数超过一定数值；还比如，同一客户端的 4xx 请求出错率太高……对于这样的一些请求访问，网关一方面要把这样的请求屏蔽掉，另一方面需要发出警告，有可能会是一些比较重大的安全问题，如被黑客攻击。

### 部署升级策略

* 停机部署（Big Bang / Recreate）： 把现有版本的服务停机，然后部署新的版本。
* 蓝绿部署（Blue/Green /Stage）：部署好新版本后，把流量从老服务那边切过来。
  * 这种部署的问题就是有点浪费，因为需要使用双倍的资源（不过，这只是在物理机时代，在云计算时代没事，因为虚拟机部署完就可以释放了）。
* 滚动部署（Rolling Update / Ramped）： 一点一点地升级现有的服务。
  * 在发布过程中，会出现新老两个版本同时在线的情况，同一用户的请求可能在新老版中切换而导致问题。 
  * 我们的新版程序没有在生产线上经过验证就上线了。  
  * 在整个过程中，生产环境处于一个新老更替的中间状态，如果有问题要回滚就有点麻烦了。  
  * 如果在升级过程中，需要做别的一些运维工作，我们还要判断哪些结点是老版本的，哪些结点是新版本的。这太痛苦了。  
  * 因为新老版本的代码同时在线，所以其依赖的服务需要同时处理两个版本的请求，这可能会带来兼容性问题。
  * 而且，我们无法让流量在新老版本中切换。    
* 灰度部署（Canary）：把一部分用户切到新版本上来，然后看一下有没有问题。如果没有问题就继续扩大升级，直到全部升级完成。
  * 灰度部署是指逐渐将生产环境流量从老版本切换到新版本。通常流量是按比例分配的。例如 90% 的请求流向老版本，10% 的请求流向新版本。然后没有发现问题，就逐步扩大新版本上的流量，减少老版本上的流量。
  * 除了切流量外，对于多租户的平台，例如云计算平台，灰度部署也可以将一些新的版本先部署到一些用户上，如果没有问题，扩大部署，直到全部用户。一般的策略是，从内部用户开始，然后是一般用户，最后是大客户。
* AB 测试（A/B Testing）：同时上线两个版本，然后做相关的比较。
  * AB 测试是同时上线两个版本，然后做相关的比较。它是用来测试应用功能表现的方法，例如可用性、受欢迎程度、可见性等。
  * AB 测试是对新版的功能没信心。注意，一个是质量，一个是功能。
  * 要观测用户的一些数据指标，这完全也可能做成自动化的，如果新版本数据好，就自动化地切一点流量过来，如果不行，就换一批用户（样本）再试试。


## 性能设计

### 缓存

绝大多数情况下，select 是出现性能问题最大的地方。一方面，select 会有很多像 join、group、order、like 等这样丰富的语义，而这些语义是非常耗性能的；另一方面，大多数应用都是读多写少，所以加剧了慢查询的问题。

* Cache Aside 更新模式
  * 失效：应用程序先从 Cache 取数据，如果没有得到，则从数据库中取数据，成功后，放到缓存中。
  * 命中：应用程序从 Cache 中取数据，取到后返回。
  * 更新：先把数据存到数据库中，成功后，再让缓存失效。

我们需要有一个爬虫保护机制，或是我们引导这些人去使用我们提供的外部 API。在那边，我们可以有针对性地做多租户的缓存系统（也就是说，把用户和第三方开发者的缓存系统分离开来）。

### 异步处理

异步通讯的设计模式有助于提高系统的稳定性和容错能力。

操作系统并不会真正同步地去操作硬盘，而是把硬盘读写请求先在内存中 hold 上一小会儿（几十毫秒），然后，对这些读写请求做 merge 和 sort。

也就是说，merge 是把相同的操作合并，相同的读操作只读一次，相同的写操作，只写最后一次，而 sort 是把不同的操作排个序，这样可以让硬盘向一个方向转一次就可以把所有的数据读出来，而不是来来回回地转。这样可以极大地提高硬盘的吞吐率。

再如，我们的 TCP 协议向网络发包的时候，会把我们要发的数据先在缓冲区中进行囤积，当囤积到一定尺寸时（MTU），才向网络发送，这样可以最大化利用我们的网络带宽。而传输速度和性能也会变得很快。

异步系统所带来的好处——让我们的系统可以统一调度


最终一致性

* 凭证需要非常好地保存起来，不然会导致事务做不下去。
* 凭证处理的幂等性问题，不然在重试时就会出现多次交易的情况。
* 如果事务完成不了，需要做补偿事务处理。

### 数据库扩展

读写分离 CQRS

分库分表 Sharding

我们需要从业务上来优化。一方面，简化业务，不要在数据库上做太多的关联查询，而对于一些更为复杂的用于做报表或是搜索的数据库操作，应该把其移到更适合的地方。比如，用 ElasticSearch 来做查询，用 Hadoop 或别的数据分析软件来做报表分析。


关于分库的策略。我们把数据库按某种规则分成了三个库。比如，或是按地理位置，或是按日期，或是按某个范围分，或是按一种哈希散列算法。总之，我们把数据分到了三个库中。

关于数据访问层。为了不让我们前面的服务感知到数据库的变化，我们需要引入一个叫 " 数据访问层 " 的中间件，用来做数据路由。但是，老实说，这个数据访问层的中间件很不好写，其中要有解析 SQL 语句的能力，还要根据解析好的 SQL 语句来做路由。但即便是这样，也有很多麻烦事。

为了避免数据访问层的麻烦，分片策略一般如下。
* 按多租户的方式。用租户 ID 来分，这样可以把租户隔离开来。比如：一个电商平台的商家中心可以按商家的 ID 来分。
* 按数据的种类来分。比如，一个电商平台的商品库可以按类目来分，或是商家按地域来分。
* 通过范围来分。这样分片，可以保证在同一分片中的数据是连续的，于是我们数据库操作，比如分页查询会更高效一些。一般来说，大多数情况是用时间来分片的，比如，一个电商平台的订单中心是按月份来分表的，这样可以快速检索和统计一段连续的数据。
* 通过哈希散列算法来分（比如：主键 id % 3 之类的算法。）此策略的目的是降低形成热点的可能性（接收不成比例的负载的分片）。但是，这会带来两个问题，一个就是前面所说的跨库跨表的查询和事务问题，另一个就是如果要扩容需要重新哈希部分或全部数据。

上面是最常见的分片模式，但是你还应考虑应用程序的业务要求及其数据使用模式。这里请注意几个非常关键的事宜。
* 数据库分片必须考虑业务，从业务的角度入手，而不是从技术的角度入手，如果你不清楚业务，那么无法做出好的分片策略。
* 请只考虑业务分片。请不要走哈希散列的分片方式，除非有个人拿着刀把你逼到墙角，你马上就有生命危险，你才能走哈希散列的分片方式。

有两种分片模式，一种是水平分片，一种是垂直分片。

### 秒杀

100 万用户被放过了 0.02% 的用户，也就是 200 个左右，而这 200 个人在数据中心抢那 100 个 iPhone，也就是 200 TPS，这个并发量怎么都应该能扛住了。

12306 最好的应对方式，除了不要一次把所有的票放出来，而是分批在不同的时间段把票放出来，这样可以让人们不要集中在一个时间点来抢票，做到人肉分流，可以降低一些并发度。

12306 最好是用预售的方式，让大家把自己的购票先输入到系统中。系统并不真正放票，而是把大家的需求都收集好，然后做整体统筹安排，该增加车次的增加车次，该加车厢的加车厢，这样可以确保大家都能走。实在不行，那就抽签了。

### 边缘计算

苹果手机的人脸识别需要到服务器上算，然后把结果返回，那么用户的体验就很糟糕了。这就是为什么苹果在手机里直接植入了神经网络的芯片。

我们可以看到，数量越来越大，分析结果的速度需要越来越快，这两个需求，只会把我们逼到边缘计算上去。 


* 处理一些实时响应的业务。它和用户靠得很近，所以可以实时响应用户的一些本地请求，比如，某公司的人脸门禁系统、共享单车的开锁。
* 处理一些简单的业务逻辑。比如像秒杀、抢红包这样的业务场景。
* 收集并结构化数据。比如，把视频中的车牌信息抠出来，转成文字，传回数据中心。
* 实时设备监控。主要是线下设备的数据采集和监控。
* P2P 的一些去中心化的应用。比如：边缘结点作为一个服务发现的服务器，可以让本地设备之间进行 P2P 通讯。
* 云资源调度。边缘结点非常适合用来做云端服务的调度。比如，允许用户使用不同生产商的云存储服务，使用不同生产商但是功能相同的 API 服务（比如支付 API 相关）。因为是流量接入方，所以可以调度流量。
* 云资源聚合。比如，我们可以把语音转文字的 API 和语义识别的 API 相结合，聚合出来一个识别语音语义的 API，从而简化开发人员的开发成本。

## 程序员练级攻略

### 开篇

学习编程还是要多多动手。

* 比较重要的几个方面
  * 英文能力
  * 提问的能力
  * 写代码的修养
  * 安全防范意识
  * 软件工程和上线规范
  * 编程规范

各种技术方向不是鱼和熊掌，是可以兼得的；很多技术是相通的，关键是你是学在表面还是深入本质。

DRY- 避免重复原则、KISS- 简单原则、迪米特法则（又称“最少知识原则”）、 面向对象的 S.O.L.I.D 原则

算法与数据结构、操作系统原理、编译原理、数据库原理、计算机原理……这些原理上的东西，是你想要成为一个专家必须要学的东西。

* 一定要坚持，要保持长时间学习，甚至终生学习的态度。
* 一定要动手，不管例子多么简单，建议至少自己动手敲一遍看看是否理解了里头的细枝末节。
* 一定要学会思考，思考为什么要这样，而不是那样。还要举一反三地思考。
* 不要乱买书，不要乱追新技术新名词，基础的东西经过很长时间积累，会在未来至少 10 年通用。
* 回顾一下历史，看看历史时间线上技术的发展，你才能明白明天会是什么样的。

对于那些你集中精力深度研究过的东西——并伴随着你的兴趣的东西——你会收获到扎实的知识。

如果你能把你的工作做好，那么其它的超级牛人都会想要和你一起工作，因为他们可以从你这学到东西，而你又可以从他们那里学到东西。

永远和高手一起工作。如果你面试的公司的面试太简单了，那就不要去，因为简单的面试通常意味着平庸。去那样的公司工作只会让你的学习速度变慢，甚至倒退。

很多时候，你缺少的不是知识而是热情。


### 零基础启蒙

前端基础。要系统地学习一下前端的知识，也就是 CSS、HTML 和 JavaScript 这三个东西。

花2-3年打实基础，坚持学习基础原理，别看他们一开始跑的快，但是没有后劲，这跟长跑和登山一样，后半程随便秒他们。

用go可以替C/C++，Java写业务代码，目前还取代不掉，除非业务简单。

如果你想成为架构师，还是学Java。

### 程序员修养

技术修养不够的话，你学再多的知识也是没有用的。

* Bad architecture causes more problems than bad code.
* You will spend more time thinking than coding.
* The best programmers are always building things.
* There’s always a better way.
* Code reviews by your peers will make all of you better.
* Fewer features for better code is always the right answer in the end.
* If it’s not tested, it doesn’t work.
* Don’t reinvent the wheel, library code is there to help.
* Code that’s hard to understand is hard to maintain.
* Code that’s hard to maintain is next to useless.
* Always know how your business makes money, that determines who gets paid what.
* If you want to feel important as a software developer, work at a tech company.

英文能力

### 编程语言

Go 语言现在很受关注，它是取代 C 和 C++ 的另一门有潜力的语言。C 语言太原始了，C++ 太复杂了，Java 太高级了，所以 Go 语言就在这个夹缝中出现了。这门语言已经 10 多年了，其已成为云计算领域事实上的标准语言，尤其是在 Docker/Kubernetes 等项目中。Go 语言社区正在不断地从 Java 社区移植各种 Java 的轮子过来，Go 社区现在也很不错。

负载均衡

底层的lvs，上层的nginx和haproxy

### 理论学科

算法、数据结构、网络模型、计算机原理等这样的计算机科学专业需要学习的理论知识是必须要学习的。

业务上我需要用算法比较两个数组中差异的布隆过滤器，或是在做监控系统时实时计算过去一分钟的 P99 统计时的蓄水池算法，或是数据库的 B+ 树索引，还有 Linux 内核中的 epoll 的红黑树，还有在做服务调度里的“背包问题”等都会用算法，真的是会本质上帮助到你，也是会让你瞬间会产生成就感的事情。

这些理论知识可以说是计算机科学这门学科最精华的知识了，认真学习，理解其背后的逻辑和思维方式，会让你受益匪浅。

### 系统知识

你需要开始需要术业有专攻了。下面给一些建议的方向。

底层方向：操作系统、文件系统、数据库、网络……
架构方向：分布式系统架构、微服务、DevOps、Cloud Native……
数据方向：大数据、机器学习、人工智能……
前端方向：你对用户体验或是交互更感兴趣，那么你走前端的路吧。
其它方向：比如，安全开发、运维开发、嵌入式开发……

### 分布式架构入门

* 服务调度，涉及服务发现、配置管理、弹性伸缩、故障恢复等。
* 资源调度，涉及对底层资源的调度使用，如计算资源、网络资源和存储资源等。
* 流量调度，涉及路由、负载均衡、流控、熔断等。
* 数据调度，涉及数据复本、数据一致性、分布式事务、分库、分表等。
* 容错处理，涉及隔离、幂等、重试、业务补偿、异步、降级等。
* 自动化运维，涉及持续集成、持续部署、全栈监控、调用链跟踪等。

这里有几点需要你注意一下。
* 分布式系统之所以复杂，就是因为它太容易出错了。这意味着，你要把处理错误的代码当成正常功能的代码来处理。
* 开发一个健壮的分布式系统的成本是单体系统的几百倍甚至几万倍。这意味着，我们要自己开发一个，需要能力很强的开发人员。
* 非常健壮的开源的分布式系统并不多，或者说基本没有。这意味着，如果你要用开源的，那么你需要 hold 得住其源码。
* 管理或是协调多个服务或机器是非常难的。这意味着，我们要去读很多很多的分布式系统的论文。
* 在分布式环境下，出了问题是很难 debug 的。这意味着，我们需要非常好的监控和跟踪系统，还需要经常做演练和测试。
* 在分布式环境下，你需要更科学地分析和统计。这意味着，我们要用 P90 这样的统计指标，而不是平均值，我们还需要做容量计划和评估。
* 在分布式环境下，需要应用服务化。这意味着，我们需要一个服务开发框架，比如 SOA 或微服务。
* 在分布式环境下，故障不可怕，可怕的是影响面过大，时间过长。这意味着，我们需要花时间来开发我们的自动化运维平台。





